<!DOCTYPE html>
<html>
<head>
  <title>Quantile Regression</title>
  <meta charset="utf-8">
  <meta name="description" content="Quantile Regression">
  <meta name="author" content="Minzhao Liu">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
    <link rel="stylesheet" href = "libraries/widgets/bootstrap/css/bootstrap.css">
<link rel="stylesheet" href = "libraries/widgets/bootstrap/css/bootstrap.min.css">
<link rel="stylesheet" href = "assets/css/ribbons.css">

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
    <!-- END LOGO SLIDE -->
    

    <!-- TITLE SLIDE -->
    <!-- Should I move this to a Local Layout File? -->
    <slide class="title-slide segue nobackground">
      <hgroup class="auto-fadein">
        <h1>Quantile Regression</h1>
        <h2>Ph. D. Dissertation Proposal</h2>
        <p>Minzhao Liu<br/>Supervisor: Dr. Mike Daniels. Department of Statistics, University of Florida</p>
      </hgroup>
      
      <footer class = 'license'>
        <a href='http://creativecommons.org/licenses/by-nc-sa/3.0/'>
        <img width = '80px' src = 'http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png'>
        </a>
      </footer>
    </slide>

    <!-- SLIDES -->
      <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>Outline</h2>
  </hgroup>
  <article>
    <ol>
<li>Introduction and Review (8-9 / 49)</li>
<li>Bayesian Quantile Regression Using Polya Trees Priors (8/49)</li>
<li>Quantile Regression in the Presence of Monotone Missingness with Sensitivity Analysis (19/49)</li>
<li>Future Work (9/49)</li>
<li>References</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Why Quantile Regression</h2>
  </hgroup>
  <article>
    
<div style='float:left;width:48%;' class='centered'>
  <p><img src="assets/img/engel.png" alt="engel"></p>


</div>
<div style='float:right;width:48%;'>
  <h3>Scatterplot, Quantile Regression Fit and Linear Regression Fit of the  Engel Food Expenditure Data:</h3>

<ul>
<li>Scatterplot of the Engel data on food expenditure vs household income for a sample of 235 19th century working class Belgian households.</li>
<li>Fitted quantile lines for quantile (0.05; 0.1; 0.25; 0.75; 0.9;
0.95).</li>
<li>Fitted median regression line</li>
<li>Dashed red line is the fitted value from linear regression model
estimates.</li>
</ul>

</div>
  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>Introduction of Quantile Regression</h2>
  </hgroup>
  <article>
    <h3>Quantile</h3>

<p>\[
Q_{Y}(\tau) = \inf \{y: F(y) \geq \tau \},
\]</p>

<h3>Quantile Regression</h3>

<p>\[
Q_{Y}(\tau|\mathbf x) = \mathbf x' \beta(\tau).
\]</p>

<h3>Quantile Regression vs Mean Regression</h3>

<ol>
<li>More information about the relationship of covariates and responses</li>
<li>Slope may varies for different quantiles</li>
<li>May focus on certain quantiles as estimates of interest</li>
<li>More complete description of the conditional distribution</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>Traditional Frequentist Methods</h2>
  </hgroup>
  <article>
    <ul>
<li>R package <code>quantreg</code> (<a href="http://CRAN.R-project.org/package=quantreg">Koenker, 2012</a>)</li>
<li>Using simplex for linear programming problems mentioned
in <a href="http://www.jstor.org/stable/1913643">Koenker et al. (1978)</a></li>
</ul>

<p>\[
\mathbf \beta(\tau) = \arg \min_b \sum_{i=1}^{n} \rho_{\tau}(y_{i} - \mathbf x_{i}' b)
\]</p>

<h2>Pros and Cons</h2>

<ul>
<li>No distributional assumptions</li>
<li>Fast using linear programming</li>
<li>Asymptotic inference may not be accurate for small sample sizes</li>
<li>Easy to generalize:

<ul>
<li>Random effect</li>
<li>\(L_1\) , \(L_2\) penalties</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Bayesian Methods</h2>
  </hgroup>
  <article>
    <ul>
<li><a href="http://dx.doi.org/10.1111/j.0006-341X.1999.00477.x">Walker &amp; Mallick (1999)</a>: diffuse finite Polya Tree</li>
<li><a href="">Yu &amp; Moyeed (2001)</a>: asymmetric Laplace distribution</li>
<li><a href="">Hanson &amp; Johnson (2002)</a>: mixture of polya tree prior for median regression on survival time in AFT model</li>
<li><a href="">Kottas &amp; Krnjajic (2009)</a>: semi-parametric models using DP
mixtures for the error distribution</li>
<li><a href="">Reich et al. (2010)</a>: an infinite mixture of Gaussian densities for error</li>
<li><a href="http://dx.doi.org/10.1080/00949655.2010.496117">Kozumi &amp; Kobayashi (2011)</a>: developed a simple and efficient Gibbs
  sampling algorithm for fitting quantile regression based on a
  location-scale mixture representation of ALD</li>
<li>Sanchez et al (2013) proposed efficient and easy EM algorithm to obtain MLE for ALD settings from the hierarchical representation of ALD</li>
</ul>

<!-- \[ -->

<!-- \rho_{\tau}(x) = x(\tau - I(x < 0)) -->

<!-- \] -->

<!-- Optimization problem for the loss function with quantile: -->

<!-- \[ -->

<!-- E \rho_{\tau} (X - \hat{x}) \implies F(\hat{x}) = \tau -->

<!-- \] -->

<!-- If $F$ is replaced by the empirical distribution function: -->

<!-- \[ -->

<!-- F_n(x) = n^{-1} \sum_{i=1}^n I(X_i \leq x) -->

<!-- \] -->

<!-- Then (2) changes to minimizing -->

<!-- \[ -->

<!-- \int \rho_\tau(X-\hat{x}) d \, F_n(x) = n^{-1} \sum_{i=1}^n \rho_\tau(X_i - \hat{x} -->

<!-- \] -->

<!-- --- &twocol -->

<!-- ## Analog from mean regression -->

<!-- *** left -->

<!-- * sample mean -->

<!-- \[ \min_{\mu \in \mathbb{R}} \sum_{i=1}^n (y_i - \mu)^2 \] -->

<!-- * least square mean regression -->

<!-- \[ \min_{\beta \in \mathbb{R}^p} \sum_{i=1}^n (y_i - \mathbf x_i^T \beta)^2 \] -->

<!-- *** right -->

<!-- * $\tau^{th}$ sample quantile -->

<!-- \[ \min_{\alpha \in \mathbb{R}} \sum_{i=1}^n \rho_\tau(y_i - \alpha) \] -->

<!-- * $\tau^{th}$ quantile regression -->

<!-- \[ \min_{\beta \in \mathbb{R}^p} \sum_{i=1}^n \rho_\tau(y_i - \mathbf x_i^T \beta) \] -->

<!-- --- -->

<!-- ## Bayesian Approach: Asymmetric Laplace Distribution -->

<!-- \[f_{\epsilon}(x) \sim ALD -->

<!-- \] -->

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-6" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <!-- ## ALD (Yu & Moyeed (2001)) -->

<!-- <div class="alert alert-info"> -->

<!-- <p> Definition: -->

<!-- A random variable $Y$ is distributed as an Asymmetric Laplace Distribution with -->

<!-- location parameter $\mu$, scale parameter $\sigma > 0$ and skewness parameter -->

<!-- $\tau \in (0, 1)$ if its pdf is given by -->

<!-- \[ -->

<!-- f(y|\mu, \sigma, \tau) = \frac{\tau (1 - \tau)}{\sigma} \exp \left\{ - \rho_{\tau} -->

<!-- \left( \frac{y  - \mu}{ \sigma} \right) \right\}. -->

<!-- \] -->

<!-- where $\rho_\tau (.)$ is the check (or loss) function -->

<!-- </p> -->

<!-- </div> -->

<!-- Property of $ALD(\mu, \sigma, \tau)$: -->

<!-- - mode at $\mu$ -->

<!-- - $P_Y(Y \le \mu) = \tau$ -->

<!-- --- -->

<!-- ## Mixture representation of ALD for efficient Gibbs sampling -->

<!-- --- -->

<!-- ## DP, PT, mixture of DP and PT -->

<!-- --- -->

<h2>Common Issues</h2>

<ul>
<li>Single quantile regression each time</li>
<li>Densities have their restrictive mode at the quantile of interest,
which is not appropriate when extreme quantiles are being investigated</li>
<li>quantile lines monotonicity constraints and difficulty in making inference for quantile
regression parameters for an interval</li>
<li>Joint inference is poor in borrowing information through single quantile regressions</li>
<li>Not coherent to pool from every individual quantile regression, because the sampling distribution of \(Y\) for \(\tau_1\)
is usually different from that under quantile \(\tau_2\) since they are assuming different error distribution
under two different quantile regressions (<a href="">Tokdar &amp; Kadane, 2011</a>)</li>
</ul>

<ul class = "build">
<li>Goal of Chapter 1 of the thesis</li>
</ul>

<!-- --- -->

<!-- ## Solution -->

<!-- - Tokdar and Kadane 2011: simultaneous linear quantile regression -->

<!-- - non-parametric model for the error term (density estimation) to avoid the monotonicity problem (Scaccia and Green 2003, -->

<!--   Geweke and Keane 2007, Taddy and Kottas 2010) -->

<!-- --- -->

<!-- ## Background of Missing Data -->

<!-- --- -->

<!-- ## Goals of the Dissertation -->

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>Chapter 1: Bayesian Quantile Regression Using Polya Trees Priors</h2>
  </hgroup>
  <article>
    <ul>
<li>[ ] May insert a Polya Tree picture here</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>Intuition</h2>
  </hgroup>
  <article>
    <p>Consider heterogeneous linear regression model from He et al (1998):</p>

<p>\[ y_i = \mathbf x_i \mathbf \beta + (\mathbf{x_i \gamma} )\epsilon_i \]</p>

<p>The \(\tau^{th}\) quantile regression parameters is</p>

<p>\[ \mathbf \beta(\tau) = \mathbf \beta + F^{-1}_\epsilon (\tau) \mathbf \gamma
\]</p>

<ul>
<li>Homogeneous model (\(\mathbf \gamma = (1, \mathbf 0)\)): parallel quantile lines</li>
<li>Heterogeneous model (\(\mathbf \gamma \neq (1, \mathbf 0)\)): non-parallel quantile lines</li>
<li>Heterogeneous linear regression model allows non-parallel quantile lines</li>
<li>[ ] Illustrate with 2 pictures</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Idea</h2>
  </hgroup>
  <article>
    <p>\[ y_i = \mathbf x_i \mathbf \beta + (\mathbf{x_i \gamma} )\epsilon_i \]</p>

<p>\[ \mathbf \beta(\tau) = \mathbf \beta + F^{-1}_\epsilon (\tau) \mathbf \gamma \]</p>

<ul>
<li>Estimate \(\mathbf \beta, \mathbf \gamma, F^{-1}_\epsilon(\tau) |\mathbf Y\), then \(\mathbf \beta(\tau) | \mathbf Y\)</li>
<li>Use mixture of Polya Tree priors to nonparametrically estimate  \(F^{-1}_\epsilon(\tau) |\mathbf Y\)</li>
<li>Closed form for predictive quantile regression parameters</li>
<li>Polya tree is a very flexile way to model the unknown distribution</li>
<li>Exact inference through MCMC and fewer assumptions</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Polya Tree Definition</h2>
  </hgroup>
  <article>
    <ul>
<li>Polya Tree priors were introduced decades ago (Freedman 1963, Fabius 1964, Ferguson 1974)</li>
<li>Lavine (1992, 1994) extended to Polya Tree models, completed definitions, and introduced how
to sample from Polya Trees</li>
<li>Advantage over Dirichlet process:

<ul>
<li>can be absolutely continuous with probability 1</li>
<li>can be easily tractable</li>
<li>Dirichlet process is just a special case of Polya Tree</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Basic</h2>
  </hgroup>
  <article>
    <p>Denote</p>

<ul>
<li>\(E=\{0,1\}\)</li>
<li>\(E^m\) as the m-fold product of \(E\)</li>
<li>\(E^0 = \emptyset\)</li>
<li>\(E^* = \cup_0^\infty E^m\)</li>
<li>\(\Omega\) be a separable measurable space</li>
<li>\(\Pi_0 = \Omega\)</li>
<li>\(\Pi=\{\Pi_m: m=0,1,...\}\) be a separating binary tree of partitions of \(\Omega\)</li>
<li>\(B_{\emptyset} = \Omega\)</li>
<li>\(\forall \epsilon=\epsilon_1\cdots \epsilon_m \in E^{*}\), \(B_{\epsilon 0}\) and
\(B_{\epsilon 1}\) are the two partition of \(B_{\epsilon}\).</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>Definition (continue)</h2>
  </hgroup>
  <article>
    <div class="alert alert-info">
<p> Polya Tree: </p>

<p>
A random probability measure $G$ on $(\Omega, \mathcal{F})$ is said to have a Polya
tree distribution, or a Polya tree prior with parameter $(\Pi, \mathcal{A})$, written
as $G|\Pi, \mathcal{A} \sim PT (\Pi, \mathcal{A})$, if there exists nonnegative number
$\mathcal{A} = \left\{ \alpha_\epsilon, \epsilon \in E^* \right \}$ and random vectors
$\mathcal{Y} = \{ Y_\epsilon : \epsilon \in E^* \}$ such that the following hold:
</p>
</div>

<ul>
<li>all the random variables in \(\mathcal{Y}\) are
independent;</li>
<li>\(Y_{\epsilon}= (Y_{\epsilon 0}, Y_{\epsilon 1}) \sim
\mathrm{Dirichlet}(\alpha_{\epsilon 0 }, \alpha_{\epsilon 1}),
\forall \epsilon \in E^{*}\);</li>
<li>\(\forall m=1,2, \ldots\), and \(\forall \epsilon \in E^{*},
G(B_{\epsilon_{1}, \ldots, \epsilon_m}) = \prod_{j=1}^m
Y_{\epsilon_1 \cdots \epsilon_j}\).</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>Polya Tree Parameters (\(\mathcal{A}\))</h2>
  </hgroup>
  <article>
    <p>Usually a Polya tree is centered around a
pre-specified distribution \(G_0\), which is called the baseline
measure.</p>

<p>\(\mathcal{A}\) determines how much \(G\) can deviate from \(G_0\).</p>

<ul>
<li>Ferguson (1974) pointed out \(\alpha_{\epsilon} = 1\) yields a \(G\) that is absolutely continuous with probability 1</li>
<li>\(\alpha_{\epsilon_1, \ldots, \epsilon_m} = m^2\) yields \(G\) that is
absolutely continuous with probability 1.</li>
<li>Walker and Mallick (1999) and
Paddock (1999) considered \(\alpha_{\epsilon_1, \ldots,
\epsilon_m} = cm^2\), where \(c > 0\).</li>
<li>Berger and Guglielmi (2001) considered
\(\alpha_{\epsilon_1, \ldots, \epsilon_m} = c \rho(m)\). In general, any
$\rho(m) $ such that \(\sum_{m=1}^{\infty} \rho(m)^{-1} < \infty\)
guarantees \(G\) to be absolutely continuous.</li>
<li>In our case, we adopt \(\alpha_{\epsilon_1, \ldots, \epsilon_m} = cm^2\).</li>
<li>\(m\) is the number of levels</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>Polya Tree Parameters (\(\Pi\))</h2>
  </hgroup>
  <article>
    <p>Partition parameter \(\Pi\)</p>

<ul>
<li><p>Pre-specified distribution \(G_0\), baseline measure</p></li>
<li><p>Canonical way of constructing a Polya Tree distribution \(G\) centering on \(G_0\)</p></li>
<li><p>\(B_0 = G^{-1}_0 ([0, 1/2]), B_1 = G^{-1}_0 ((1/2,1])\)</p></li>
<li><p>\(G(B_0) = G(B_1)= 1/2\)</p></li>
<li><p>\(\forall \epsilon \in E^{*}\), choose
\(B_{\epsilon 0 }\) and \(B_{\epsilon 1}\) to satisfy
\(G(B_{\epsilon 0 } |B_{\epsilon} ) = G(B_{\epsilon 1} | B_{\epsilon}) = 1/2\)</p></li>
<li><p>A simple example is to choose \(B_{\epsilon 0}\) and \(B_{\epsilon 1}\) in level \(m\) by setting them as
\(G^{-1}_0 \left((k/2^m, (k+1)/2^m] \right)\), for \(k=0,..., 2^m-1\).</p></li>
</ul>

<p>(<em>May show picture here</em>)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>Properties of Polya Tree</h2>
  </hgroup>
  <article>
    <h3>Expectation of Polya Tree</h3>

<p>Suppose \(G \sim PT(\Pi, \mathcal{A})\) is a random probability
measure and \(Y_1, Y_2, ...\) are random samples from \(G\).</p>

<p>\(F= E(G)\) as a probability measure is defined by
\(F(B) = E(G(B)),\forall B \in \mathcal{B}\). By the definition of Polya tree, for
  any \(\epsilon \in E^{*}\),
\[
    F(B_{\epsilon})  = E(G(B_{\epsilon})) = \prod_{j=1}^m
    \frac{\alpha_{\epsilon_1, \ldots, \epsilon_j}}{\alpha_{\epsilon_1,
        \ldots, \epsilon_{j-1},0} + \alpha_{\epsilon_1, \ldots, \epsilon_{j-1},1}}.
\]</p>

<p>If \(G\) is constructed based on baseline measure \(G_0\) and we set
\(\alpha_{\epsilon_1, ..., \epsilon_m} = cm^2\),
\(\alpha_{\epsilon_0 }= \alpha_{\epsilon_1}\), then
\(\forall B \in \mathcal{B}, F(B) = G_0(B)\); thus, \(F=G_0\), if there is no data.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>Density Function</h2>
  </hgroup>
  <article>
    <p>Suppose \(F=E(G), G|\Pi, \mathcal{A} \sim PT (\Pi, \mathcal{A})\),
where $G_0 $ is the baseline measure. Then, using the canonical
construction, \(F=G_0\), the density function is</p>

<p>\[
f(y) = \left[ \prod_{j=1}^m \frac{ \alpha_{\epsilon_1, \ldots, \epsilon_j}(y)}{\alpha_{\epsilon_1, \ldots, \epsilon_{j-1},0}(y) +\alpha_{\epsilon_1, \ldots, \epsilon_{j-1},1}(y)} \right] 2^{m } g_0(y)
\]
where \(g_0\) is the pdf of \(G_0\).</p>

<p>When using the canonical construction with no data,
\(\alpha_{\epsilon_0 } = \alpha_{\epsilon_1}\), above equation
simplifies to</p>

<p>\[
f(y) = g_0(y).
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>Conjugacy</h2>
  </hgroup>
  <article>
    <p>If \(y_1, ..., y_n | G \sim G, G|\Pi,\mathcal{A} \sim PT(\Pi, \mathcal{A})\),
then \(G|y_1, ..., y_n, \Pi, \mathcal{A} \sim PT(\Pi, \mathcal{A}^{*})\), where in
\(\mathcal{A}^{*}, \forall \epsilon \in E^{*}\),</p>

<p>\[
    \alpha_{\epsilon}^{*} = \alpha_{\epsilon} + n_{\epsilon}(y_1, \ldots, y_n),
\]
where \(n_{\epsilon}(y_1, ..., y_n)\) indicates the count of how many
samples of \(y_1, ..., y_n\) fall in \(B_{\epsilon}\).</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>Mixture of Polya Trees</h2>
  </hgroup>
  <article>
    <ul>
<li>The behavior of a single Polya tree highly depends on how the
partition is specified.</li>
<li>A random probability measure \(G_\theta\) is
said to be a mixture of Polya tree if there exists a random
variable \(\theta\) with distribution \(h_{\theta}\), and Polya tree
parameters \((\Pi^{\theta}, \mathcal{A}^{\theta})\) such that</li>
</ul>

<p>\[
G_{\theta} | \theta=\theta \sim PT (\Pi^{\theta}, \mathcal{A}^{\theta})
\]</p>

<div class="alert alert-info">
<p> Example:
Suppose $G_0 = \mathrm{N}(\mu, \sigma^2)$ is the baseline measure.
For $\epsilon \in E^{*}, \alpha_{\epsilon_m} = cm^2 $,
$\mathbf \theta = (\mu, \sigma, c)$ is the mixing index and the distribution on
$\Theta = (\mu, \sigma, c) $ is the mixing distribution.
</p>
</div>

<ul>
<li>With the mixture of Polya tree, the influence of the partition is
lessened</li>
<li>Inference will not be affected greatly by a single
Polya tree distribution.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h2>Predictive Error Density (1)</h2>
  </hgroup>
  <article>
    <ul>
<li>Suppose \(G_{\theta}\) is the baseline measure, \(g_0(y)\) is the density
function.</li>
<li>\(\Pi^{\theta}\) is defined as
\[
B^{\theta}_{\epsilon_1, \ldots, \epsilon_m} = \left( G^{-1}_{\theta}
\left( \frac{k}{2^m} \right), G^{-1}_{\theta}\left( \frac{k+1}{2^m} \right) \right),
\]
where \(k\) is the index of partition \(\epsilon_1, \ldots, \epsilon_m\)
in level \(m\).</li>
<li>\(\mathcal{A}^c\) is defined as
\[
\alpha_{\epsilon_1, \ldots, \epsilon_m} = cm^2.
\]
Therefore, the error model is
\[
\begin{aligned}
y_1, \ldots, y_n |G_{\theta} & \sim G, \\
G|\Pi^{\theta}, \mathcal{A}^{c} & \sim PT (\Pi^{\theta},
\mathcal{A}^{c}).
\end{aligned}
\]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>Predictive Error Density (2)</h2>
  </hgroup>
  <article>
    <p>The predictive density function of \(Y|y_1, \ldots, y_n, \theta\),
marginalizing out \(G\), is
\[
  f_Y^{\theta} (y|y_1, \ldots, y_n)  = \lim_{m \to \infty} \left(
    \prod_{j=2}^m \frac{cj^2 + n_{\epsilon_1 \cdots \epsilon_j(x) }(y_1, \ldots, y_n)}{2cj^2
      + n_{\epsilon_1 \cdots \epsilon_{j-1}(x)}(y_1, \ldots, y_n)}
  \right)2^{m-1} g_0(y),
\]
where \(n_{\epsilon_1 \cdots \epsilon_j(x) }(y_1, \ldots, y_n)\)
denotes the number of observations \(y_1, \ldots, y_n\) dropping in the
bin \(\epsilon_1 \cdots \epsilon_j\) where \(y\) stays in the level
\(j\).</p>

<ul>
<li>If we restrict the first level weight as
\(\alpha_0=\alpha_1=1\), then we only need to update levels beyond
the first level.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-21" style="background:;">
  <hgroup>
    <h2>Finite Polya Tree</h2>
  </hgroup>
  <article>
    <ul>
<li><p>In practice, a finite \(M\) level Polya Tree is usually adopted to
approximate the full Polya tree, in which, only up to \(M\) levels
are updated.</p></li>
<li><p>The corresponding predictive density becomes
\[
f_Y^{\theta, M} (y|y_1, \ldots, y_n)  =  \left(
  \prod_{j=2}^M \frac{cj^2 + n_{\epsilon_1 \cdots \epsilon_j(x) }(y_1, \ldots, y_n)}{2cj^2
    + n_{\epsilon_1 \cdots \epsilon_{j-1}(x)}(y_1, \ldots, y_n)}
\right)2^{M-1} g_0(y).
\]</p></li>
<li><p>The rule of thumb for choosing \(M\) is to set \(M=\log_2n\), where \(n\)
is the sample size (Hanson et al 2002)</p></li>
<li><p><a href="">Hanson &amp; Johnson (2002)</a> showed the approximation
is exact for \(M\) large enough.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-22" style="background:;">
  <hgroup>
    <h2>Predictive Cumulative Density Function</h2>
  </hgroup>
  <article>
    <p>Based on the predictive density function of a
finite Polya tree distribution, the predictive cumulative density
function is
\[
    F^{\theta,M}_Y(y|y_1, \ldots, y_n) = \sum_{i=1}^{N-1} P_{i} + P_N
    \left( G_{\theta}(y)2^M -(N-1) \right),
\]
where
\[
\begin{aligned}
    P_i &= \frac{1}{2} \left(\prod_{j=2}^M \frac{cj^2 + n_{j,\lceil
          i2^{j-M} \rceil}(y_1, \ldots, y_n)}{2cj^2 + n_{j-1,\lceil
          i2^{j-1-M} \rceil}(y_{1 },\ldots, y_n)} \right) \mbox{ and}\\
    N & = \left[ 2^{M } G_{\theta}(y) +1\right],
\end{aligned}
\]
in which \(n_{j,\lceil i2^{j-M} \rceil}(y_1, \ldots, y_n)\) denotes
the number of observations \(y_1, \ldots, y_n\) in the \(\lceil
  i2^{j-M} \rceil\) slot at level \(j\), \(\lceil \cdot \rceil\) is the
  ceiling function, and \([ \cdot ]\) is the floor function.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-23" style="background:;">
  <hgroup>
    <h2>Predictive Error Quantiles</h2>
  </hgroup>
  <article>
    <ul>
<li><p>The posterior predictive quantile of finite Polya tree
distribution is
\[
Q^{\theta, M}_{Y|y_1, \ldots, y_n}(\tau) = G^{-1}_{\theta} \left(
  \frac{\tau- \sum_{i=1}^N P_i + N P_N}{2^M P_N} \right),
\]
where \(N\) satisfies \(\sum_{i=1}^{N-1} P_i < \tau \le \sum_{i=1}^N P_i\).</p></li>
<li><p>The explicit form for quantile regression coefficients becomes:
\[
\mathbf{\beta}(\tau) = \mathbf{\beta} + \mathbf{\gamma}G_{\theta}^{-1}
\left(\frac{\tau - \sum_{i=1}^NP_i +
  NP_N}{2^MP_N}  \right),
\]</p></li>
<li><p>Greatly facilitate computations</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-24" style="background:;">
  <hgroup>
    <h2>Method</h2>
  </hgroup>
  <article>
    <h3>Fully Bayesian Quantile Regression Specification with Mixture of Polya Tree Priors</h3>

<p>The full Bayesian specification of quantile regression is given as
follows,
\[
\begin{align*}
  y_i& = \mathbf{x_i'\beta} + (\mathbf{x_i'\gamma}) \epsilon_{i}, i = 1,
  \ldots,
  n \\
  \epsilon_i |G_{\theta} & \sim G_{\theta} \\
  G_{\theta}|\Pi^{\theta}, \mathcal{A}^{\theta} & \sim PT
  (\Pi^{\theta}, \mathcal{A}^{\theta}) \\
  \mathbf{\theta} = (\sigma, c) & \sim \pi_{\mathbf \theta}(\mathbf \theta) \\
  \mathbf{\beta} & \sim \pi_{\mathbf \beta}(\mathbf \beta)\\
  \mathbf{\gamma} &\sim \pi_{\mathbf \gamma}(\mathbf \gamma).
\end{align*}
\]
In order to not confound the location parameter, $\epsilon_i $ or \(G\)
is set to have median 0 by fixing \(\alpha_0=\alpha_1 = 1\). For the
similar reason, the first component of \(\mathbf{\gamma}\) is fixed at 1.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-25" style="background:;">
  <hgroup>
    <h2>Posterior Distribution of (\(\mathbf{\beta}, \mathbf \gamma, \sigma, c\))</h2>
  </hgroup>
  <article>
    <p>\[
  \begin{aligned}
    P(\mathbf{\beta}, \mathbf{\gamma}, \sigma, c|\mathbf{Y}) & \propto L(\mathbf{Y}|
    \mathbf{\beta}, \mathbf{\gamma}, \sigma, c) \pi_{\beta}(\beta)
    \pi_{\gamma}(\gamma) \pi_{\sigma}(\sigma) \pi_c(c) \\
    & = \frac{1}{\prod_{i=1}^n (\mathbf{x_i'\gamma})} P \left(
      \epsilon_1, \ldots, \epsilon_n | \mathbf{\beta}, \mathbf{\gamma},
      \sigma, c\right) \pi_{\beta}(\beta)
    \pi_{\gamma}(\gamma) \pi_{\sigma}(\sigma) \pi_c(c) \\
    & = \frac{1}{\prod_{i=1}^n (\mathbf{x_i'\gamma})} P
    \left(\epsilon_n| \epsilon_1, \ldots, \epsilon_{n-1}, \mathbf{\beta},
      \mathbf{\gamma}, \sigma, c\right) \cdots P \left(\epsilon_2|
      \epsilon_1, \mathbf{\beta}, \mathbf{\gamma}, \sigma, c\right) P
    \left(\epsilon_1| \mathbf{\beta}, \mathbf{\gamma},
      \sigma, c\right)\\
    & \qquad \pi_{\mathbf{\beta}}(\mathbf{\beta})
    \pi_{\mathbf{\gamma}}(\mathbf{\gamma}) \pi_{\sigma}(\sigma) \pi_c(c),
  \end{aligned}
\]
where \(\epsilon_i = (y_i - \mathbf{x_i'\beta})/(\mathbf{x_i'\gamma})\)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-26" style="background:;">
  <hgroup>
    <h2>Priors</h2>
  </hgroup>
  <article>
    <h3>\((\sigma, c)\)</h3>

<p>Diffuse gamma prior:
\[
\begin{align*}
  \pi(\sigma) & \sim \Gamma (1/2, 1/2), \\
  \pi(c) & \sim \Gamma(1/2, 1/2).
\end{align*}
\]</p>

<h3>\((\mathbf \beta, \mathbf \gamma)\)</h3>

<p>\[
\begin{align*}
  \pi_{\mathbf \beta} (\beta_j) &= \delta_{\beta_j} \phi(\beta_j; 0, s_j^2
  \sigma_{\beta_j}^2) +
  (1- \delta_{\beta_j}) \phi(\beta_j; \beta_j^p, \sigma_{\beta_j}^2),\\
  \delta_{\beta_j} & \sim \mbox{Bernoulli} (\pi_{\beta_j}),
\end{align*}
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-27" style="background:;">
  <hgroup>
    <h2>Spike and Slab Priors</h2>
  </hgroup>
  <article>
    <ul>
<li>Shrink toward zero</li>
<li>Do variable selection on both quantile regression parameters and heterogeneity parameters</li>
<li>Improve efficiency</li>
<li>Use continuous spike and slab priors on each component of \((\mathbf \beta, \mathbf \gamma)\) (<a href="http://www.jstor.org/stable/2290777">George &amp; McCulloch, 1993</a>)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-28" style="background:;">
  <hgroup>
    <h2>Spike and Slab Priors (Continued)</h2>
  </hgroup>
  <article>
    <p>The density function of priors for
\(\beta_j\) can be written as:
\[
\begin{aligned}
  \pi_{\mathbf \beta} (\beta_j) &= \delta_{\beta_j} \phi(\beta_j; 0, s_j^2\sigma_{\beta_j}^2) +
  (1- \delta_{\beta_j}) \phi(\beta_j; \beta_j^p, \sigma_{\beta_j}^2),\\
  \delta_{\beta_j} & \sim \mbox{Bernoulli} (\pi_{\beta_j}),
  \end{aligned}
\]</p>

<ul>
<li>\(\phi(x; \mu, \sigma^2)\) is the density function of normal
distribution at \(x\) with mean \(\mu\) and variance
\(\sigma^2\).</li>
<li>\(\beta_j^p, \sigma_{\beta_j}^2\) are the mean and variance
of the diffuse normal prior for the slab component.</li>
<li>\(\delta_{\beta_j}\) is the indicator that \(\beta_j\) comes from spike
component or from slab component and \(\pi_{\beta_j}\) is its corresponding
probability.</li>
<li>\(s_j (>0)\) is small enough</li>
<li>\(\delta_{\beta_j} = 1\), it indicates \(|\beta_j | < 3 s_j\sigma_{\beta_j}\) with high
probability, thus it can be approximately estimated as 0 and regarded
as non-significant and removed from the model</li>
<li>\(\delta_{\beta_j} =0\), it indicates \(\beta_j\) comes from the slab component, thus
\(\beta_j\) is believed to come from a diffuse prior distribution</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-29" style="background:;">
  <hgroup>
    <h2>Choice of Diffuse Priors</h2>
  </hgroup>
  <article>
    <p>\[
\begin{aligned}
  \pi_{\mathbf \beta} (\beta_j) &= \delta_{\beta_j} \phi(\beta_j; 0, s_j^2\sigma_{\beta_j}^2) +
  (1- \delta_{\beta_j}) \phi(\beta_j; \beta_j^p, \sigma_{\beta_j}^2),\\
  \delta_{\beta_j} & \sim \mbox{Bernoulli} (\pi_{\beta_j}),
  \end{aligned}
\]</p>

<ul>
<li>We choose \(\mathbf \beta^p\), the mean of normal distribution of slab
component, to be least square estimates of \(\mathbf Y\) given covariates
matrix \(\mathbf X\), i.e., \(\mathbf{(X^TX)^{-1}X^TY}\).</li>
<li><p>Let
\(\sigma_{\beta_j}^2\) be the diagonal component of matrix
\(\hat{\sigma}^2 \mathbf{(X^TX)^{-1}}\),
where \(\hat{\sigma}^2 = \sum_i^n (y_i - \mathbf{x_i\beta}^p)^2/(n - p)\).</p></li>
<li><p>The priors for \(\mathbf \gamma\) are similar to priors for \(\mathbf \beta\).</p></li>
<li><p>\(\mathbf \gamma^p = \mathbf 0\)</p></li>
<li><p>\(\mathbf \sigma_{\gamma} = \mathbf 100\)</p></li>
<li><p>To shrink heterogeneity parameters toward 0</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-30" style="background:;">
  <hgroup>
    <h2>Choice of \(\pi_\beta\), \(\pi_\gamma\)</h2>
  </hgroup>
  <article>
    <p>\[
\begin{aligned}
  \pi_{\mathbf \beta} (\beta_j) &= \delta_{\beta_j} \phi(\beta_j; 0, s_j^2\sigma_{\beta_j}^2) +
  (1- \delta_{\beta_j}) \phi(\beta_j; \beta_j^p, \sigma_{\beta_j}^2),\\
  \delta_{\beta_j} & \sim \mbox{Bernoulli} (\pi_{\beta_j}),
  \end{aligned}
\]</p>

<ul>
<li>The \(\pi_{\beta_j}\) and \(\pi_{\gamma_j}\) control the belief that the
corresponding regressors are needed in the model.</li>
<li>Large \(\pi\) reflects
doubt that regressors should be included, and vice versa.</li>
<li>Furthermore,
we can put hyper priors on \(\pi_{\beta_j}\) and \(\pi_{\gamma_j}\) to get
rid of uncertainty about distribution of the components.</li>
<li>For example,
in this article, we assign priors for \(\pi_{\beta_j}\) and
\(\pi_{\gamma_j}\) to be a beta distribution with parameters \((1,1)\).</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-31" style="background:;">
  <hgroup>
    <h2>Computation Details</h2>
  </hgroup>
  <article>
    <ul>
<li>Using an MCMC algorithm implemented in our R package <em>bqrpt</em></li>
<li>Draw posterior samples of (\(\mathbf \beta, \mathbf \gamma, \sigma, c | \mathbf
Y\))</li>
<li>Adaptive Metropolis-Hasting algorithm</li>
<li>Thinning</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-32" style="background:;">
  <hgroup>
    <h2>Metropolis-Hasting Algorithm</h2>
  </hgroup>
  <article>
    <ul>
<li>Candidate distribution

<ul>
<li>\(\beta_j^{*} \sim N(\beta_j^{l-1}, t_{\beta_j} (\mathbf{X'X})^{-1}_{jj})\)</li>
<li>\(\gamma_j^* \sim N(\gamma_j^{l-1}, t_{\gamma_j}(\mathbf{X'X})^{-1}_{jj})\)</li>
<li>\(\sigma^* \sim LogNormal(\log \sigma^{l-1}, t_{\sigma})\)</li>
<li>\(c^* \sim LogNormal(\log c^{l-1}, t_c)\)</li>
</ul></li>
<li>Adaptive Metropolis-Hasting algorithm

<ul>
<li>\(t_{\beta_j}, t_{\gamma_j}, t_{\sigma}, t_c\) are the tuning parameters to adjust acceptance rate (<a href="">Jara et al. 2009</a>)</li>
<li>For good MCMC mixing performance, we adjust the acceptance rate of the
adaptive Metropolis-Hasting algorithm to around 0.2 for sampling</li>
<li>Tuning parameters are increased(decreased) by
multiplying(dividing) \(\delta(l) = \exp(\min(0.01, l^{-1/2}))\) when
current acceptance proportion is larger(smaller) than target optimal
acceptance rate for every 100 iterations during burn-in period, where
\(l\) is the number of current batches of 100 iterations</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-33" style="background:;">
  <hgroup>
    <h2>Thinning</h2>
  </hgroup>
  <article>
    <ul>
<li>When the actual error distribution is far away from
the Polya tree baseline measure, the MCMC trace plot may reflect strong
autocorrelation among posterior samples. Thus we recommend thinning
to reduce the autocorrelation.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-34" style="background:;">
  <hgroup>
    <h2>Simulation</h2>
  </hgroup>
  <article>
    <ul>
<li><em>RQ</em>: rq function in (<a href="http://CRAN.R-project.org/package=quantreg">Koenker, 2012</a>)
(frequentist quantile regression method)</li>
<li><em>FBQR</em>: flexible Bayesian
quantile regression (<a href="">Reich et al. 2010</a>)</li>
<li><em>PT</em>: Polya trees  with normal diffuse priors</li>
<li><em>PTSS</em>: Polya trees with spike and slab priors</li>
<li>Compare for both homogeneous and heterogeneous models</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-35" style="background:;">
  <hgroup>
    <h2>Design</h2>
  </hgroup>
  <article>
    <ul>
<li>[M1:] \(y_i = 1 + x_{i1}\beta_1 + \epsilon_{1i}\), \(\epsilon_{1i} \sim N(0, 1)\)</li>
<li>[M2:] \(y_i = 1 + x_{i1}\beta_1 + \epsilon_{2i}\), \(\epsilon_{1i} \sim t_3()\)</li>
<li>[M3:] \(y_i = 1 + x_{i1}\beta_1 + \epsilon_{3i}\), \(\epsilon_{1i} \sim 0.5 N(-2,1) + 0.5N(2,1)\)</li>
<li>[M4:] \(y_i = 1 + x_{i1}\beta_1 + \epsilon_{4i}\), \(\epsilon_{1i} \sim 0.8 N(0,1) + 0.2N(3,3)\)</li>
<li>[M1H:] \(y_i = 1 + x_{i1}\beta_1 + (1 + 0.2x_{i1})
\epsilon_{1i}\),</li>
<li>[M2H:] \(y_i = 1 + x_{i1}\beta_1 + (1 + 0.2x_{i1})
\epsilon_{2i}\),</li>
<li>[M3H:] \(y_i = 1 + x_{i1}\beta_1 + (1 + 0.2x_{i1})
\epsilon_{3i}\),</li>
<li>[M4H:] \(y_i = 1 + x_{i1}\beta_1 + (1 + 0.2x_{i1})
\epsilon_{4i}\),</li>
<li>[M5:] \(y_{i} | R_i = 1 \sim 2 + x_{i1} + \epsilon_{1i}, y_{i}|
R_i = 0 \sim -2 - x_{i1} + \epsilon_{1i}\), \(\epsilon_{1i} \sim N(0, 1)\)</li>
<li>\(x_{i1} \sim \mathrm{Uniform}(0,4)\)</li>
<li>\(\beta_1 = 1\)</li>
<li>\(n = 200\)</li>
<li>100 data sets</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-36" style="background:;">
  <hgroup>
    <h2>Explain</h2>
  </hgroup>
  <article>
    <ul>
<li>(explain these models) In model 1 (M1), the error distribution
coincides with baseline distribution. Model 2 (M2) has a heavier tail
distribution, student-t distribution with 3 degrees of freedom. Model
3 (M3) has a bimodal distribution for the error term.  Model 4 (M4)
uses a skewed mixture of normal distribution error introduced in
<a href="">Reich et al. (2010)</a>. Model 1H-4H (M1H-M4H) assume heterogeneous
variances such that the quantiles lines are no long parallel to each
other. Model 5 (M5) also assumes heterogeneous variance, but the
heterogeneity comes from the mixture of distributions instead of
heterogeneous variance from covariates.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-37" style="background:;">
  <hgroup>
    <h2>PT Priors</h2>
  </hgroup>
  <article>
    <p>\[
\begin{align*}
  \pi(\beta_j) & \sim N(\mathbf \beta_j^p, \mathbf V_{jj}) , j = 0, 1,\\
  \pi(\gamma_j) & \sim N(0, 100), j = 1,\\
  \pi(\sigma) & \sim \Gamma (a/2, b/2), \\
  \pi(c) & \sim \Gamma(a/2, b/2),
\end{align*}
\]</p>

<ul>
<li>\(\mathbf \beta^p = \mathbf{(X'X)^{-1}X'Y}\) is the least square
estimator</li>
<li>\(\mathbf V = \hat{\sigma}^2\mathbf{(X'X)^{-1}}\)</li>
<li>\(\hat{\sigma}^2 = \sum_{i = 1}^n (y_i - \mathbf {x_i \beta^p})^2/ (n - 3)\),</li>
<li>\(a = b = 1\).</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-38" style="background:;">
  <hgroup>
    <h2>PTSS Priors</h2>
  </hgroup>
  <article>
    <ul>
<li>Same priors for \(\sigma\) and \(c\)</li>
<li>spike-slab priors for \(\mathbf \beta\) and \(\mathbf \gamma\):
\[
\begin{align*}
\pi(\beta_j) & \sim \delta_{\beta_j}N(0, s_j\mathbf V_{jj}) +  (1 - \delta_{\beta_j})N(\mathbf \beta_j^p, \mathbf V_{jj}) , j = 0, 1, \\
\pi(\gamma_j) & \sim \delta_{\gamma_j}N(0, 100s_j) + (1 - \delta_{\gamma_j}) N(0, 100), j = 1, \\
\delta_{\beta_j} & \sim \mbox{Bernoulli}(\pi_{\beta_j}) , \pi_{\beta_j} \sim \mbox{Beta}(1, 1),\\
\delta_{\gamma_j} & \sim \mbox{Bernoulli}(\pi_{\gamma_j}),
\pi_{\gamma_j} \sim \mbox{Beta}(1, 1).
\end{align*}
\]</li>
<li>\(s_j = 1/1000\) from <a href="http://www.jstor.org/stable/2290777">George &amp; McCulloch (1993)</a></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-39" style="background:;">
  <hgroup>
    <h2>MCMC Setup</h2>
  </hgroup>
  <article>
    <ul>
<li>\(M = 7\)</li>
<li>30,000 burn-in</li>
<li>30,000 saved samples</li>
<li>thin: 5</li>
<li>Acceptance rates were set to approach 20% for all parameters candidates during the adaptive Metropolis-Hastings algorithm</li>
<li>It takes around 90 seconds for one simulation for PT under R version 2.15.3 (2013-03-01) and
platform: x86_64-apple-darwin9.8.0/x86_64 (64-bit).</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-40" style="background:;">
  <hgroup>
    <h2>Evaluation Methods</h2>
  </hgroup>
  <article>
    <ul>
<li><p><strong>MSE</strong>
\[
\mbox{MSE}  =  \frac{1}{N}\sum_{i = 1}^N (\hat{\beta}_j(\tau) -
\beta_j(\tau))^2 ,
\]</p>

<ul>
<li>\(N\) is the number of simulations</li>
<li>\(\beta_j(\tau)\) is the \(j^{th}\) component of the true quantile regression
parameters</li>
<li>\(\hat{\beta}_j(\tau)\) is the \(j^{th}\) component of
estimated quantile regression parameters</li>
<li>We use the posterior
mean as estimated parameters.</li>
</ul></li>
<li><p><strong>Monte Carlo standard errors (MCSE)</strong> are used to evaluate the
<em>significance</em> of the differences between methods,
\[
\mbox{MCSE} = \hat{\mbox{sd}}(\mbox{Bias}^2)/\sqrt{N},
\]</p>

<ul>
<li>\(\hat{\mbox{sd}}\) is the sample standard deviation</li>
<li>\(\mbox{Bias} = \hat{\beta}_{j}(\tau) - \beta_{j}(\tau)\).</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-41" style="background:;">
  <hgroup>
    <h2>Simulation Results</h2>
  </hgroup>
  <article>
    <p>Mean squared error (reported as 100<em>average) and MCSE
(reported as 100*MCSE) for each
quantile regression method.   The four columns (RQ, FBQR,
PT, PTSS) stand for frequentist method *rq</em> function from
<em>quantreg</em> R package, flexible Bayesian method by Reich, and
our Bayesian approach using Polya tree with normal priors and with
spike and slab priors.}</p>

<table><thead>
<tr>
<th>Term</th>
<th>RQ</th>
<th>FBQR</th>
<th>PT</th>
<th>PTSS</th>
<th>RQ</th>
<th>FBQR</th>
<th>PT</th>
<th>PTSS</th>
</tr>
</thead><tbody>
<tr>
<td></td>
<td>M1 50%</td>
<td></td>
<td></td>
<td></td>
<td>M1H 50%</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\(\beta_0\)</td>
<td>2.55(0.39)</td>
<td>1.69(0.23)</td>
<td>1.70(0.23)</td>
<td>1.70(0.23)</td>
<td>3.05(0.60)</td>
<td>2.38(0.42)</td>
<td>2.41(0.40)</td>
<td>2.42(0.39)</td>
</tr>
<tr>
<td>\(\beta_1\)</td>
<td>0.52(0.08)</td>
<td>0.31(0.04)</td>
<td>0.31(0.04)</td>
<td>0.31(0.04)</td>
<td>0.84(0.18)</td>
<td>0.54(0.11)</td>
<td>0.60(0.11)</td>
<td>0.60(0.11)</td>
</tr>
<tr>
<td></td>
<td>M1 90%</td>
<td></td>
<td></td>
<td></td>
<td>M1H 90%</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\(\beta_0\)</td>
<td>7.68(0.98)</td>
<td>4.89(0.70)</td>
<td>3.91(0.54)</td>
<td>3.39(0.48)</td>
<td>9.53(1.82)</td>
<td>6.90(12.93)</td>
<td>4.80(1.02)</td>
<td>5.76(1.02)</td>
</tr>
<tr>
<td>\(\beta_1\)</td>
<td>1.31(0.16)</td>
<td>0.84(0.12)</td>
<td>0.73(0.10)</td>
<td>0.60(0.08)</td>
<td>2.33(0.41)</td>
<td>1.60(0.43)</td>
<td>1.33(0.26)</td>
<td>1.49(0.27)</td>
</tr>
</tbody></table>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-42" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <table><thead>
<tr>
<th>Term</th>
<th>RQ</th>
<th>FBQR</th>
<th>PT</th>
<th>PTSS</th>
<th>RQ</th>
<th>FBQR</th>
<th>PT</th>
<th>PTSS</th>
</tr>
</thead><tbody>
<tr>
<td></td>
<td>M2 50%</td>
<td></td>
<td></td>
<td></td>
<td>M2H 50%</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\(\beta_0\)</td>
<td>3.41(0.41)</td>
<td>2.67(0.37)</td>
<td>2.83(0.38)</td>
<td>2.77(0.36)</td>
<td>4.23(0.53)</td>
<td>2.84(0.31)</td>
<td>4.54(0.54)</td>
<td>4.92(0.58)</td>
</tr>
<tr>
<td>\(\beta_1\)</td>
<td>0.61(0.07)</td>
<td>0.49(0.06)</td>
<td>0.51(0.07)</td>
<td>0.51(0.06)</td>
<td>0.96(0.15)</td>
<td>0.73(0.10)</td>
<td>1.23(0.17)</td>
<td>1.30(0.17)</td>
</tr>
<tr>
<td></td>
<td>M2 90%</td>
<td></td>
<td></td>
<td></td>
<td>M2H 90%</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\(\beta_0\)</td>
<td>18.12(3.31)</td>
<td>11.95(2.01)</td>
<td>16.09(2.72)</td>
<td>11.93(2.27)</td>
<td>32.76(6.52)</td>
<td>15.09(2.40)</td>
<td>20.73(4.04)</td>
<td>30.90(4.37)</td>
</tr>
<tr>
<td>\(\beta_1\)</td>
<td>3.64(0.57)</td>
<td>1.84(0.22)</td>
<td>3.39(0.47)</td>
<td>2.00(0.31)</td>
<td>8.35(1.30)</td>
<td>3.70(0.62)</td>
<td>7.95(1.43)</td>
<td>5.62(1.20)</td>
</tr>
<tr>
<td></td>
<td>M3 50%</td>
<td></td>
<td></td>
<td></td>
<td>M3H 50%</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\(\beta_0\)</td>
<td>82.04(8.61)</td>
<td>16.60(2.36)</td>
<td>9.02(1.40)</td>
<td>13.68(2.29)</td>
<td>98.56(9.87)</td>
<td>16.49(2.09)</td>
<td>10.33(1.29)</td>
<td>16.28(2.12)</td>
</tr>
<tr>
<td>\(\beta_1\)</td>
<td>17.68(1.73)</td>
<td>1.79(0.33)</td>
<td>1.21(0.20)</td>
<td>1.19(0.19)</td>
<td>26.90(2.98)</td>
<td>2.88(0.36)</td>
<td>2.06(0.29)</td>
<td>1.80(0.25)</td>
</tr>
<tr>
<td></td>
<td>M3 90%</td>
<td></td>
<td></td>
<td></td>
<td>M3H 90%</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\(\beta_0\)</td>
<td>10.86(1.34)</td>
<td>6.64(0.99)</td>
<td>9.26(1.53)</td>
<td>8.99(1.45)</td>
<td>13.19(1.89)</td>
<td>9.11(1.28)</td>
<td>12.29(1.70)</td>
<td>12.38(1.75)</td>
</tr>
<tr>
<td>\(\beta_1\)</td>
<td>2.16(0.30)</td>
<td>1.41(0.17)</td>
<td>1.76(0.26)</td>
<td>1.68(0.24)</td>
<td>3.94(0.51)</td>
<td>2.61(0.36)</td>
<td>3.58(0.50)</td>
<td>3.56(0.51)</td>
</tr>
</tbody></table>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-43" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <table><thead>
<tr>
<th>Term</th>
<th>RQ</th>
<th>FBQR</th>
<th>PT</th>
<th>PTSS</th>
<th>RQ</th>
<th>FBQR</th>
<th>PT</th>
<th>PTSS</th>
</tr>
</thead><tbody>
<tr>
<td></td>
<td>M4 50%</td>
<td></td>
<td></td>
<td></td>
<td>M4H 50%</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\(\beta_0\)</td>
<td>5.74(0.75)</td>
<td>4.26(0.61)</td>
<td>6.47(0.87)</td>
<td>6.46(0.90)</td>
<td>5.24(0.74)</td>
<td>5.09(0.65)</td>
<td>6.66(0.83)</td>
<td>7.06(0.95)</td>
</tr>
<tr>
<td>\(\beta_1\)</td>
<td>0.84(0.09)</td>
<td>0.61(0.08)</td>
<td>0.86(0.12)</td>
<td>0.84(0.12)</td>
<td>1.42(0.19)</td>
<td>1.14(0.15)</td>
<td>1.38(0.18)</td>
<td>1.44(0.18)</td>
</tr>
<tr>
<td></td>
<td>M4 90%</td>
<td></td>
<td></td>
<td></td>
<td>M4H 90%</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\(\beta_0\)</td>
<td>52.96(6.46)</td>
<td>23.18(3.08)</td>
<td>22.99(2.88)</td>
<td>19.64(2.52)</td>
<td>88.72(11.12)</td>
<td>37.07(4.88)</td>
<td>38.45(5.02)</td>
<td>40.52(5.67)</td>
</tr>
<tr>
<td>\(\beta_1\)</td>
<td>11.35(1.39)</td>
<td>3.10(0.36)</td>
<td>3.83(0.48)</td>
<td>2.79(0.36)</td>
<td>25.00(3.17)</td>
<td>7.09(0.94)</td>
<td>7.46(0.94)</td>
<td>8.04(1.00)</td>
</tr>
<tr>
<td></td>
<td>M5 50%</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\(\beta_0\)</td>
<td>174.22(18.64)</td>
<td>46.01(5.92)</td>
<td>6.77(1.44)</td>
<td>4.87(1.06)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\(\beta_1\)</td>
<td>149.63(10.65)</td>
<td>10.75(1.68)</td>
<td>1.83(0.61)</td>
<td>1.58(0.33)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>M5 90%</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\(\beta_0\)</td>
<td>8.10(1.07)</td>
<td>6.68(0.97)</td>
<td>11.48(1.48)</td>
<td>13.05(1.69)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>\(\beta_1\)</td>
<td>1.54(0.22)</td>
<td>2.03(0.28)</td>
<td>2.77(0.36)</td>
<td>2.96(0.40)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>

<p>(Explain)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-44" style="background:;">
  <hgroup>
    <h2>Simulation Summary</h2>
  </hgroup>
  <article>
    <ul>
<li><p>When error distribution  coincides with Polya tree baseline measure, RQ
has a larger MSE than FBQR and PT. PTSS performed best since its prior
shrunk the heterogeneity parameters toward zero.  When considering
heterogeneity in model 1 (M1H), PT and PTSS still perform well versus
RQ and FBQR.</p></li>
<li><p>In model 2, 3, 4 and 2H, 3H, 4H, when error is homogeneous or
heterogeneous, and is from a mixture of normals (student t
distribution can be regarded as a mixture of normals), which is away
from Polya tree baseline measure, FBQR dominates the other three
methods in terms of MSE, because simulated models coincide with the
models in the FBQR approach. However, PT and PTSS are also
competitive.  In median regression for model 3 and model 3
heterogeneity scenario, PT and PTSS have smaller MSE than FBQR and RQ.
The similar situation also happened in model 4 with 90% quantile
regression.</p></li>
<li><p>In model 5, the heterogeneity comes from the mixture of distributions.
the mode of the error distribution is no longer at median for RQ and
FBQR, thus leading to larger MSE. Although PT and PTSS have larger MSE
than RQ and BQR in 90% quantile, the deficit is offset by much
smaller bias in 50% quantile regression.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-45" style="background:;">
  <hgroup>
    <h2>Simulation Summary</h2>
  </hgroup>
  <article>
    <ul>
<li>In all cases, the RQ method performs poorly in terms of MSE
since the mode of the error is no longer the quantile of interest.</li>
<li>In contrast, PT is not impacted by lack of unimodality and heterogeneity
and provides more information for the relationship between responses
and covariates.</li>
<li>FBQR outperforms PT in some cases, since the error is
assigned an infinite mixture of normal distribution in FBQR.</li>
<li>Less information is available from our approach to detect the shape at a
particular extreme percentile of the distribution since there are few
observations at extreme quantiles.</li>
<li>However, PT and PTSS can fit simultaneously multiple quantile regressions and provide coherent
information about the error distribution.</li>
<li>An overall evaluation method over multiple quantiles, such as summation of MSE over all
quantiles and coefficients, may reflect PT and PTSS have advantages
when error distribution is away from regular unimodal shape as in
model 3 (M3 and M3H) and model 5 (M5).</li>
<li>Quantile lines do not cross using our method.</li>
<li>Expect to see advantages when dimension of responses is bivariate or more.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-46" style="background:;">
  <hgroup>
    <h2>Real Data Analysis: Tours</h2>
  </hgroup>
  <article>
    <ul>
<li>a recent weight management study (<a href="">Perri et al. 2008</a>)</li>
<li>Was designed to test whether a lifestyle modification program could effectively
help people to manage their weights in the long term</li>
<li>We are interested in the effects of <strong>age</strong> and <strong>race</strong></li>
<li>The age of the subjects ranged from 50 to 75</li>
<li>There were 43 blacks and 181 whites</li>
<li>Our goal is to determine how the percentiles
of <strong>weight change</strong> are affected by their age and race</li>
<li><strong>Age</strong> covariate are scaled to 0 to 5 with every increment representing 5
years.</li>
<li>We fitted regression models for quantiles (10%, 30%, 50%, 70%,
90%)</li>
<li>We used Bayesian posterior samples to construct 95%
credible intervals</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-47" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <p><img src="assets/img/weight-age-race3.png" alt="weight-age"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-48" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <table><thead>
<tr>
<th>\(\tau\)</th>
<th>Term</th>
<th>PT</th>
<th>PTSS</th>
<th>RQ</th>
<th>FBQR</th>
</tr>
</thead><tbody>
<tr>
<td>10%</td>
<td>Intercept</td>
<td>2.62(1.11,4.22)</td>
<td>2.10(0.65,3.36)</td>
<td>2.20(1.39,4.63)</td>
<td>1.90(0.04,3.62)</td>
</tr>
<tr>
<td></td>
<td>Age</td>
<td>-0.57(-1.25,-0.03)</td>
<td>-0.57(-1.09,-0.07)</td>
<td>-0.25(-0.73,0.16)</td>
<td>-0.32(-0.99,0.36)</td>
</tr>
<tr>
<td></td>
<td>Race</td>
<td>2.70(1.20,4.29)</td>
<td>3.32(2.07,4.70)</td>
<td>2.40(-0.23,3.92)</td>
<td>2.92(0.91,5.06)</td>
</tr>
<tr>
<td>30%</td>
<td>Intercept</td>
<td>5.59(4.64,6.70)</td>
<td>5.45(4.41,6.36)</td>
<td>5.56(4.83,6.52)</td>
<td>5.32(3.67,6.80)</td>
</tr>
<tr>
<td></td>
<td>Age</td>
<td>-0.46(-0.91,-0.10)</td>
<td>-0.47(-0.82,-0.19)</td>
<td>-0.66(-1.28,0.05)</td>
<td>-0.47(-1.02,0.05)</td>
</tr>
<tr>
<td></td>
<td>Race</td>
<td>3.38(2.22,4.42)</td>
<td>3.58(2.56,4.65)</td>
<td>3.74(2.04,4.42)</td>
<td>3.56(1.99,5.20)</td>
</tr>
<tr>
<td>50%</td>
<td>Intercept</td>
<td>7.43(6.46,8.56)</td>
<td>7.47(6.24,8.40)</td>
<td>7.83(5.42,9.09)</td>
<td>7.55(6.07,9.13)</td>
</tr>
<tr>
<td></td>
<td>Age</td>
<td>-0.40(-0.75,-0.08)</td>
<td>-0.42(-0.72,-0.16)</td>
<td>-0.57(-1.04,0.14)</td>
<td>-0.50(-1.06,0.03)</td>
</tr>
<tr>
<td></td>
<td>Race</td>
<td>3.81(2.77,4.68)</td>
<td>3.74(2.76,4.72)</td>
<td>3.53(2.52,5.46)</td>
<td>3.89(2.36,5.33)</td>
</tr>
<tr>
<td>70%</td>
<td>Intercept</td>
<td>9.79(8.74,11.09)</td>
<td>10.12(8.92,11.18)</td>
<td>9.70(7.95,12.39)</td>
<td>9.84(8.11,11.83)</td>
</tr>
<tr>
<td></td>
<td>Age</td>
<td>-0.31(-0.74,0.06)</td>
<td>-0.34(-0.74,0.00)</td>
<td>-0.69(-1.12,0.20)</td>
<td>-0.57(-1.16,0.04)</td>
</tr>
<tr>
<td></td>
<td>Race</td>
<td>4.35(3.19,5.39)</td>
<td>3.94(2.87,4.99)</td>
<td>4.80(2.11,6.61)</td>
<td>4.30(2.59,5.75)</td>
</tr>
<tr>
<td>90%</td>
<td>Intercept</td>
<td>12.80(11.30,14.62)</td>
<td>13.53(11.98,15.06)</td>
<td>12.61(11.48,15.27)</td>
<td>13.65(11.65,15.86)</td>
</tr>
<tr>
<td></td>
<td>Age</td>
<td>-0.20(-0.89,0.38)</td>
<td>-0.24(-0.86,0.30)</td>
<td>-0.71(-1.59,-0.05)</td>
<td>-0.55(-1.38,0.42)</td>
</tr>
<tr>
<td></td>
<td>Race</td>
<td>5.05(3.36,6.61)</td>
<td>4.21(2.85,5.51)</td>
<td>6.08(2.48,6.85)</td>
<td>4.69(2.39,6.86)</td>
</tr>
</tbody></table>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-49" style="background:;">
  <hgroup>
    <h2>Tours Summary</h2>
  </hgroup>
  <article>
    <ul>
<li>Whites lost more weight than blacks for all quantiles.</li>
<li>The differential is reported as significant by PT and PTSS, and
becomes larger when comparing more successful weight losers (70\% -
90\% percentile).</li>
<li>For example, whites lost 5.05 kg more than blacks among people
losing the most weight (90\%) reported from method PT (4.21 kg from
PTSS).</li>
<li>The effect of age on the weight loss is small and not significant in
most cases (only barely significant in 10\% and 30\% quantile
regression by PT and PTSS).</li>
<li>The trend is negative showing that older people tend to lose less
weight. For example, median weight loss is 0.40 kg less for every age
increase of 5 years reported by PT.</li>
<li>PTSS tends to shrink coefficients toward zero. For example, the
posterior probability that the heterogeneity parameters are zero are
all 100\% for Age and 99\% for Race, indicating there is no
heterogeneity for covariates AGE and RACE.</li>
<li>This can help to select variables in Bayesian models. For example,
we can exclude AGE out of the regressors or conclude the variance is
homogeneous on the AGE covariate.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-50" style="background:;">
  <hgroup>
    <h2>Comparison to <em>FBQR</em> and <em>RQ</em></h2>
  </hgroup>
  <article>
    <ul>
<li><p>Results from method RQ and FBQR show similar conclusions as PT and
PTSS.  But there are still differences on the estimates and
statistical significance.</p></li>
<li><p>For 10\% quantile, RQ reports Race is not an significant factor,
which differs from the other three methods.</p></li>
<li><p>For 30\% quantile, Age is not significant factor on weight loss from
RQ and FBQR. However, both PT and PTSS report Age has a significant
negative effect.</p></li>
<li><p>For 50\% and 70\% quantile, the four methods have some differences
on the estimates, but they all have agreements on the significances of
the covariates.</p></li>
<li><p>For 90\% quantile, RQ method provides different results than
others. Age is reported as significant on weight loss, while the other
methods show Age does not affect the 90\% quantile for weight loss.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-51" style="background:;">
  <hgroup>
    <h2>Discussion</h2>
  </hgroup>
  <article>
    <ul>
<li>Bayesian approach for simultaneous linear quantile regression by
introducing mixture of Polya tree priors and estimating
heterogeneity parameters.</li>
<li>By marginalizing the predictive density function of the Polya
tree distribution, quantiles of interest can be obtained in closed
form by inverting the predictive cumulative distribution.</li>
<li>Exact posterior inference can be made via MCMC.</li>
<li>Here, quantile lines cannot cross since quantiles are estimated
through density estimation.</li>
<li>The simulations show our method performs better than the frequentist
approach especially when the error is multimodal and highly skewed.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-52" style="background:;">
  <hgroup>
    <h2>Future Work</h2>
  </hgroup>
  <article>
    <ul>
<li>Further research includes quantile regression for correlated data by
modelling error as a mixture of multivariate Polya tree
distribution</li>
<li>Our approach allows for quantile regression with missing data under
ignorability by adding a data augmentation step.</li>
<li>We are exploring extending our approach to allow for nonignorable
missingness.</li>
<li>It might be possible to use a slightly more complex baseline
distribution in Polya tree adaptively to improve the estimation.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-53" style="background:;">
  <hgroup>
    <h2>Multivariate Polya Tree Introduction ?</h2>
  </hgroup>
  <article>
    <ul>
<li>Paddock 1999, 2002 studied multivariate Polya Tree in a k-dimensional hypercube</li>
<li>Hanson 2006 constructed a general framework for multivariate random variable with Polya Tree distribution</li>
<li>Jara et al 2009 extended the multivariate mixture of Polya Tree prior with a directional orthogonal matrix.
And he demonstrated how to fit a generalized mixed effect model by modeling
multivariate random effects within multivariate mixture of Polya Tree priors</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-54" style="background:;">
  <hgroup>
    <h2>Chapter 2: Quantile Regression in the Presence of Monotone Missingness with Sensitivity Analysis</h2>
  </hgroup>
  <article>
    
  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-55" style="background:;">
  <hgroup>
    <h2>Review</h2>
  </hgroup>
  <article>
    <ul>
<li><a href="http://dx.doi.org/10.1093/biomet/ass007">Wei et al. (2012)</a> proposed a multiple imputation method
for quantile regression model when there are some covariates missing
at random (MAR).</li>
<li>They impute the missing covariates by specifying its conditional
density given observed covariates and outcomes, which comes from the
estimated conditional quantile regression and specification of
conditional density of missing covariates given observed ones.</li>
<li>However, they put more focus on the missing covariates rather than
missing outcomes.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-56" style="background:;">
  <hgroup>
    <h2>Review</h2>
  </hgroup>
  <article>
    <ul>
<li><a href="http://dx.doi.org/10.2427/8758">Bottai &amp; Zhen (2013)</a> illustrated an imputation method
using estimated conditional quantiles of missing outcomes given
observed data.</li>
<li>Their approach does not make distributional assumptions.</li>
<li>They assumed the missing data mechanism (MDM) is ignorable.</li>
<li>However, because their imputation method is not derived from a joint
distribution, the joint distribution with such conditionals may not
exist.</li>
<li>In addition, their approach does not allow for MNAR.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-57" style="background:;">
  <hgroup>
    <h2>Review</h2>
  </hgroup>
  <article>
    <ul>
<li><a href="http://dx.doi.org/10.1111/j.1541-0420.2009.01269.x">Yuan &amp; Yin (2010)</a> introduced a fully parametric Bayesian
quantile regression approach for longitudinal data with nonignorable
missing data.</li>
<li>They used shared latent subject-specific random effects to explain
the within-subject correlation and to associate the response process
with missing data process, and applied multivariate normal priors on
the random terms to match the traditional quantile regression check
function with penalties.</li>
<li>However, the quantile regression coefficients are conditional on the
random effects, which is not of interest if we are interested in
interpreting regression coefficients unconditional on random effects.</li>
<li>In addition, they are conditional on random effects, which tie
together the responses and missingness process, so they have slightly
different interpretation than regular random effects in longitudinal
methods.</li>
<li>Moreover, due to their full parametric specification for the full
data, their model does not allow for sensitivity analysis, which is a
key component in inference for incomplete data (NAS 2010).</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-58" style="background:;">
  <hgroup>
    <h2>Review</h2>
  </hgroup>
  <article>
    <ul>
<li>Pattern mixture models were originally proposed to model missing
data in <a href="">Rubin (1977)</a>.</li>
<li>Later mixture models were extended to handle MNAR in longitudinal
data.</li>
<li>For discrete dropout times,
<a href="http://dx.doi.org/10.1080/01621459.1993.10594302">Little (1993)</a>; <a href="http://dx.doi.org/10.1093/biomet/81.3.471">Little (1994)</a> proposed a general method
by introducing a finite mixture of multivariate distribution for
longitudinal data.</li>
<li>When there are many possible dropout time, <a href="http://dx.doi.org/10.1111/j.0006-341X.2003.00097.x">Roy (2003)</a>
proposed to group them by latent classes.</li>
<li><a href="http://dx.doi.org/10.1111/j.1541-0420.2007.00884.x">Roy &amp; Daniels (2008)</a> extended <a href="http://dx.doi.org/10.1111/j.0006-341X.2003.00097.x">Roy (2003)</a> to
generalized linear models and proposed a pattern mixture model for
data with nonignorable dropout, borrowing ideas from
<a href="http://www.jstor.org/stable/2533591">Heagerty (1999)</a>.</li>
<li>their approach only estimates the marginal covariate effects on the
mean.</li>
<li>We will use related ideas for quantile regression models which
allows non-ignorable missingness and sensitivity analysis.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-59" style="background:;">
  <hgroup>
    <h2>Notations</h2>
  </hgroup>
  <article>
    <ul>
<li>Under monotone dropout, without loss of generality,</li>
<li>denote \(S_i \in \{1, 2, \ldots, J\}\) to be the number of observed \(Y_{ij}'s\) for subject \(i\),</li>
<li>\(\mathbf Y_i = (Y_{i1}, Y_{i2}, \ldots, Y_{iJ})^{T}\) to
be the full data response vector for subject \(i\),</li>
<li>\(J\) is the maximum follow up time.</li>
<li>We assume \(Y_{i1}\) is always observed.</li>
<li>We are interested in the \(\tau\)-th marginal quantile regression coefficients
\(\mathbf \gamma_j = (\gamma_{j0}, \gamma_{j2}, \ldots, \gamma_{jp})^T\),
\[
Pr (Y_{ij} \leq \mathbf x_i^{T} \mathbf \gamma_j ) = \tau, \mbox{ for } j = 1, \ldots, J,
\]
where \(\mathbf x_i\) is a \(p \times 1\) vector of covariates for subject
\(i\).</li>
<li>Let
\[
Pr_k(Y) = Pr (Y | S = k), \quad  Pr_{\geq k} (Y)  = Pr (Y | S \geq k)
\]
be the densities of response \(\mathbf Y\) given follow-up time \(S=k\) and \(S
\geq k\). And \(Pr_k\) be the corresponding probability given \(S = k\).</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-60" style="background:;">
  <hgroup>
    <h2>Pattern Mixture Model</h2>
  </hgroup>
  <article>
    <ul>
<li>pattern mixture model jointly model the response and
missingness (<a href="http://dx.doi.org/10.1093/biomet/81.3.471">Little, 1994</a>; <a href="http://dx.doi.org/10.1201/9781420011180">Daniels &amp; Hogan, 2008</a>)</li>
<li>Mixture models factor the
joint distribution of response and missingness as
\[
p (\mathbf y, \mathbf S, |\mathbf x, \mathbf \omega) = p (\mathbf y|\mathbf S, \mathbf x, \mathbf \omega) p (\mathbf S | \mathbf x, \mathbf \omega).
\]</li>
<li>the full-data response follows the distribution is given by
\[
p (\mathbf y | \mathbf x, \mathbf \omega) = \sum_{S \in \mathcal{S}} p(\mathbf y| \mathbf S, \mathbf x, \mathbf \theta) p (\mathbf S | \mathbf x, \mathbf \phi),
\]
where \(\mathcal{S}\) is the sample space for dropout time \(S\) and the
parameter vector \(\mathbf \omega\) is partitioned as
\((\mathbf \theta, \mathbf \phi)\).</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-61" style="background:;">
  <hgroup>
    <h2>Pattern Mixture Model</h2>
  </hgroup>
  <article>
    <ul>
<li><p>Furthermore, the conditional distribution of response within patterns
can be decomposed as
\[
P (Y_{obs}, Y_{mis} | \mathbf S, \mathbf \theta) = P
(Y_{mis}|Y_{obs}, \mathbf S, \mathbf \theta_E) Pr (Y_{obs} | \mathbf S, \mathbf
\theta_{y, O}),
\]</p></li>
<li><p>where \(\mathbf \theta_E\) indexes the parameters in the extrapolation
distribution, the first term on the right hand side and \(\mathbf
\theta_{y, O}\) indexes parameters in the distribution of observed
responses, the second term on the right hand side.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-62" style="background:;">
  <hgroup>
    <h2>Model Settings</h2>
  </hgroup>
  <article>
    <ul>
<li>We assume models within pattern to be multivariate normal
distributions and specify a sequential model parametrization.</li>
<li>Let the subscript \(i\) stand for subject \(i\).</li>
<li><p>The marginal quantile regression models as:
\[
Pr (Y_{ij} \leq \mathbf x_{ij}^T \mathbf \gamma_j ) = \tau,
\]
where \(\mathbf \gamma_j\) is the \(\tau^{th}\) quantile regression
coefficients of interest for component \(j\).</p></li>
<li><p>Then we specify the conditional distributions as:
\[
\begin{array}{l}
  \displaystyle p_k(y_{i1}) = N (\Delta_{i1} + \mathbf x_{i1}^T \mathbf \beta_1^{(k)},
  \sigma_1^{(k)}  ), k = 1, \ldots, J,\\
   \displaystyle p_k(y_{ij}|\mathbf y_{ij^{-}}) =
  \begin{cases}
    \textrm{N} \big (\Delta_{ij} + \mathbf x_{ij}^T \mathbf h_{j}^{(k)} +
    \mathbf y_{ij^{-}}^T \mathbf \beta_{y,j-1}^{(k)},
    \sigma_j^{(k)} \big), & k < j ;  \\
    \textrm{N} \big (\Delta_{ij} + \mathbf y_{ij^{-}}^T \mathbf
    \beta_{y,j-1}^{(\geq j)},
    \sigma_j^{(\geq j)} \big), & k \geq j ;  \\
  \end{cases}, \mbox{ for } 2 \leq j \leq J,  \\
  \displaystyle S_{ij} = k| \mathbf x_{ij} \sim \textrm{Multinomial}(1, \mathbf \phi),
\end{array}
\]</p></li>
<li><p>where \(\mathbf y_{ij^{-}} = (y_{i1}, \ldots, y_{i(j-1)})^T\) is the
response history for subject \(i\) up to time point  \((j-1)\); \(\mathbf \phi =
(\phi_1, \ldots, \phi_J)\) is the multinomial probability vector for the
number of observed responses; \(\mathbf h_j^{(k)} = (h_{j1}^{(k)}, \ldots,
h_{jp}^{(k)})\) are sensitivity parameters .  \(\mathbf x_{ij}\) is a \(p \times 1\) covariate vector;
\(\mathbf \beta_{y, j-1}^{(k)} = \big(\beta_{y_1, j-1}^{(k)}, \ldots,
\beta_{y_{j-1}, j-1}^{(k)} \big)^T\) are autoregressive coefficients
and \(\sigma_j^{(k)}\) is the conditional standard deviation of response
component \(j\). We specify the model to have
multivariate normal distribution within patterns such that MAR exists
(<a href="http://dx.doi.org/10.1111/j.1541-0420.2011.01565.x">Wang &amp; Daniels, 2011</a>).</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-63" style="background:;">
  <hgroup>
    <h2>\(\Delta\)</h2>
  </hgroup>
  <article>
    <p>\(\Delta_{ij}\) are functions of \(\tau, \mathbf x_{ij},
\mathbf \beta, \mathbf h, \mathbf \sigma, \mathbf \gamma_j, \mathbf \phi\) and are determined by the marginal
quantile regressions,
\begin{equation}
  \label{eq:deltaeqn1}
  \tau = Pr (Y<em>{ij} \leq \mathbf x</em>{ij}<sup>T</sup> \mathbf \gamma<em>j ) = \sum</em>{k=1}<sup>J</sup>
  \phi<em>kPr_k (Y</em>{ij} \leq \mathbf x<em>{ij}<sup>T</sup> \mathbf \gamma_j ) \mbox{  for  } j = 1,
\end{equation}
and
\begin{align}\label{eq:deltaeqn2}
  \tau &amp;= Pr (Y</em>{ij} \leq \mathbf x<em>{ij}<sup>{T}</sup> \mathbf \gamma_j ) =
  \sum</em>{k=1}<sup>J</sup>
  \phi<em>kPr_k (Y</em>{ij} \leq \mathbf x<em>{ij}<sup>{T}</sup> \mathbf \gamma_j ) \
  &amp; = \sum</em>{k=1}<sup>J</sup> \phi<em>k \int\cdots \int Pr_k (Y</em>{ij} \leq \mathbf
  x<em>{ij}<sup>{T}</sup> \mathbf \gamma_j | \mathbf y</em>{ij<sup>{-}}</sup>
  ) p<em>k (y</em>{i(j-1)}| \mathbf y<em>{i(j-1)<sup>{-}})</sup>  \nonumber \
  &amp; \quad \cdots p_k (y</em>{i2}| y<em>{i1}) p_k(y</em>{i1})
  dy<em>{i(j-1)}\cdots dy</em>{i1}.  \mbox{  for  } j = 2, \ldots, J .\nonumber
\end{align}</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-64" style="background:;">
  <hgroup>
    <h2>Intuition</h2>
  </hgroup>
  <article>
    <ul>
<li>The idea is to model the marginal quantile regressions directly,</li>
<li>then to embed them in the likelihood through restrictions in the mixture
model.</li>
<li>The mixture model  allows the marginal
quantile regression coefficients to differ by quantiles. Otherwise,
the quantile lines would be parallel to each other. Moreover,</li>
<li>the mixture model also allows sensitivity analysis.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-65" style="background:;">
  <hgroup>
    <h2>Identifiability</h2>
  </hgroup>
  <article>
    <p>For identifiability of the observed data distribution, we apply the
following restrictions,
\[
 \sum_{k=1}^J \beta_{l1}^{(k)} = 0, l = 1,\ldots, p,
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-66" style="background:;">
  <hgroup>
    <h2>Missing Data Mechanism and Sensitivity Analysis</h2>
  </hgroup>
  <article>
    <p>In general, mixture models are not identified due to insufficient
information provided by observed data. Specific forms of missingness
are needed to induce constraints to identify the distributions for
incomplete patterns, in particular, the extrapolation distribution
. In this section, we explore ways to embed the
missingness mechanism and sensitivity parameters in mixture models for
our setting.</p>

<p>In the mixture model , MAR holds (<a href="http://dx.doi.org/10.1111/1467-9574.00075">Molenberghs et al. 1998</a>; <a href="http://dx.doi.org/10.1111/j.1541-0420.2011.01565.x">Wang &amp; Daniels, 2011</a>) if and only if, for each \(j \geq 2\) and \(k < j\):
\[
  p_k(y_j|y_1, \ldots, y_{j-1}) = p_{\geq j}(y_j|y_1, \ldots, y_{j-1}).
\]
When \(2 \leq j \leq J\) and \(k < j\), \(Y_j\) is not observed, thus
\(\mathbf h_j^{(k)}\) and \(\sigma_j^{(k)}\), $\mathbf \beta<em>{y,
  j-1}<sup>{(k)}</sup> = \big(\beta</em>{y<em>1,j}<sup>{(k)},</sup> \ldots,
\beta</em>{y_{j-1},j-1}<sup>{(k)}</sup> \big)<sup>T</sup> $ can not be identified from the
observed data.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-67" style="background:;">
  <hgroup>
    <h2>Sensitivity Analysis</h2>
  </hgroup>
  <article>
    <p>\[
\begin{align*}
  \log \sigma_j^{(k)} &= \log \sigma_j^{(\geq j)} +  \delta_{j}^{(k)}, \\
  \mathbf \beta_{y, j-1}^{(k)} &= \mathbf \beta_{y, j-1}^{(\geq j)} +
  \mathbf \eta_{j-1}^{(k)},
\end{align*}
\]
where \(\mathbf \eta_{j-1}^{(k)} = \big( \eta_{y_1,j-1}^{(k)}, \ldots,
\eta_{y_{j-1}, j-1}^{(k)} \big)\) for \(k < j\). Then \(\mathbf \xi_s = (
\mathbf h_j^{(k)}, \mathbf \eta_{j-1}^{(k)}, \delta_j^{(k)})\) is a set
of sensitivity parameters (<a href="http://dx.doi.org/10.1201/9781420011180">Daniels &amp; Hogan, 2008</a>), where $k &lt; j, 2 \leq j \leq
J $.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-68" style="background:;">
  <hgroup>
    <h2>Sensitivity Analysis</h2>
  </hgroup>
  <article>
    <p>When \(\mathbf \xi_s = \mathbf \xi_{s0} = \mathbf 0\), MAR holds. If
\(\mathbf \xi_s\) is fixed at \(\mathbf \xi_s \neq \mathbf \xi_{s0}\), the
missingness mechanism is MNAR. We can vary \(\mathbf \xi_s\) around
\(\mathbf 0\) to examine the impact of different MNAR mechanisms.</p>

<p>For fully Bayesian inference, we can put priors on \((\mathbf \xi_s,
\mathbf \xi_m)\) as :
\[
  p(\mathbf \xi_s, \mathbf \xi_m) = p(\mathbf \xi_s) p(\mathbf \xi_m),
\]
where \(\mathbf \xi_m = \big(\mathbf \gamma_j, \mathbf \beta_{y,
  j-1}^{(\geq j)}, \mathbf \alpha_j^{(\geq j)}, \mathbf \phi \big)\), the
identified parameters in the data distribution.  If we assume MAR with
no uncertainty, the prior of \(\mathbf \xi_s\) is \(p(\mathbf \xi_s =
\mathbf 0) \equiv 1\). Sensitivity analysis can be executed by putting
point mass priors on \(\mathbf \xi_s\) to examine the effect of priors on
the posterior inference about quantile regression coefficients \(\mathbf
\gamma_{j}^{\tau}\). For example, if MAR is assumed with uncertainty,
priors can be assigned as \(\textrm{E}(\mathbf \xi_s) = \mathbf \xi_{s0}
= \mathbf 0\) with \(\textrm{Var}(\mathbf \xi_s) \neq \mathbf 0\). If we
assume MNAR with no uncertainty, we can put priors satisfying
\(\textrm{E}(\mathbf \xi_s) = \Delta_{\xi}\), where \(\Delta_{\xi} \neq
\mathbf 0\) and \(\textrm{Var}(\mathbf \xi_s) = \mathbf 0\). If MNAR is
assumed with uncertainty, then priors could be \(\textrm{E}(\mathbf
\xi_s) = \Delta_{\xi}\), where $\Delta_{\xi} \neq \mathbf 0 $ and
\(\textrm{Var}(\mathbf \xi_s) \neq \mathbf 0\).</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-69" style="background:;">
  <hgroup>
    <h2>Curse of Dimension</h2>
  </hgroup>
  <article>
    <p>In general, each pattern \(S = k\) has its own set of sensitivity
parameters \(\mathbf \xi_s^{(k)}\). However, to keep the number of
sensitivity parameters at a manageable level (<a href="http://dx.doi.org/10.1201/9781420011180">Daniels &amp; Hogan, 2008</a>) and
without loss of generality, we assume \(\mathbf \xi_s\) does not depend
on pattern.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-70" style="background:;">
  <hgroup>
    <h2>Calculation of \(\Delta\)</h2>
  </hgroup>
  <article>
    <p>\(\Delta_{ij}\) depends on subject-specific covariates \(\mathbf x_{ij}\),
thus \(\Delta_{ij}\) needs to be calculated for each subject. We now
illustrate how to calculate \(\Delta_{ij}\) given all the other
parameters \(\mathbf \xi = (\mathbf \xi_m, \xi_s)\).</p>

<p><strong>$\Delta_{i1}: $</strong> Expand equation :
\[
\begin{align*}
    \tau = \sum_{k = 1}^J \phi_k \Phi \left( \frac{\mathbf x_{i1}^T
        \mathbf \gamma_1 - \Delta_{i1} - \mathbf x_{i1}^T\mathbf
        \beta_1^{(k)}}{ \sigma_1^{(k)} } \right),
  \end{align*}
\]
  where \(\Phi\) is the standard normal CDF. Because the above equation
  is continuous and monotone in \(\Delta_{i1}\), it can be solved by a
  standard numerical root-finding method (e.g. bisection method) with
  minimal difficulty.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-71" style="background:;">
  <hgroup>
    <h2>Calculation of \(\Delta_{ij}, 2\leq j \leq J\)</h2>
  </hgroup>
  <article>
    <p>An integral of a normal CDF with mean \(b\) and standard deviation
\(a\) over another normal distribution with mean \(\mu\) and standard
deviation \(\sigma\) can be simplified to a closed form in terms of
normal CDF:</p>

<p>\[
\begin{array}{l}
\displaystyle
\int \Phi \left( \frac{x-b}{a} \right) d\Phi(x; \mu, \sigma)  =
\begin{cases}
        1- \Phi \left( \frac{b-\mu}{\sigma} \big /
            \sqrt{\frac{a^2}{\sigma^2}+1} \right) & a > 0, \\
        \Phi \left( \frac{b-\mu}{\sigma} \big /
          \sqrt{\frac{a^2}{\sigma^2}+1} \right) & a < 0,
\end{cases}
\]</p>

<p>where \(\Phi(x; \mu, \sigma)\) stands for a CDF of normal
distribution with mean \(\mu\) and standard deviation \(\sigma\).</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-72" style="background:;">
  <hgroup>
    <h2>Calculation of \(\Delta_{ij}, 2\leq j \leq J\)</h2>
  </hgroup>
  <article>
    <p>Given the result in Lemma , to solve equation
  (\ref{eq:deltaeqn2}), we propose a recursive approach. For the first
  multiple integral, apply lemma
   once to obtain:</p>

<p>\[
  \begin{align*}
    Pr_1 (Y_{ij} \leq \mathbf x_{ij}^T \mathbf \gamma_j) & =
    \int\dots\int
    Pr (Y_{ij} \leq \mathbf x_{ij}^T\mathbf \gamma_j | S=1, \mathbf x_{ij}, \mathbf Y_{ij^{-}})\\
    & \quad  dF(Y_{i(j-1)}|S=1, \mathbf x_{ij}, \mathbf Y_{i(j-1)^{-}}) \cdots d F (Y_{i1} | S = 1, \mathbf x_{ij}), \\
    & = \int\dots\int
    \Phi \left( \frac{\mathbf x^T \mathbf \gamma_j - \mu_{j|1, \ldots, j-1}(\mathbf Y_{ij^{-}})}{\sigma_{j|1, \ldots, j-1}} \right) \\
    & \quad   dF(Y_{i(j-1)}|S=1, \mathbf x_{ij}, \mathbf Y_{i(j-1)^{-}}) \cdots d F (Y_{i1} | S = 1, \mathbf x_{ij}), \\
    & = \int\dots\int \Phi \left( \frac{Y_{i(j-2)} - b^{*}}{a^{*}}
    \right) dF(Y_{i(j-2)}|S=1, \mathbf x_{ij}, \mathbf Y_{i(j-2)^{-}}) \\
    & \quad \cdots d F (Y_{i1} | S = 1, \mathbf x_{ij}).
  \end{align*}
\]</p>

<p>Then, by recursively applying lemma \((j-1)\) times,
  each multiple integral in equation  can be
  simplified to single normal CDF. Thus we can easily solve for
  \(\Delta_{ij}\) using standard numerical root-finding method as for \(j
  = 1\).</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-73" style="background:;">
  <hgroup>
    <h2>MLE</h2>
  </hgroup>
  <article>
    <p>The observed data likelihood for an individual \(i\) with follow-up time
\(S_i = k\) is
\[
\begin{align}
L_i(\mathbf \xi| \mathbf y_i, S_{i} = k) & =
  \phi_kp_k (y_k | y_1, \ldots, y_{k-1})
  p_k (y_{k-1}|y_1, \ldots, y_{k-2}) \cdots p_{k} (y_1), \\
  & = \phi_k p_{\geq k} (y_k | y_1, \ldots, y_{k-1}) p_{\geq k-1}
  (y_{k-1}|y_1, \ldots, y_{k-2}) \cdots p_{k} (y_1), \nonumber
\end{align}
\]
where \(\mathbf y_i = (y_1, \ldots, y_k)\).</p>

<p>We use derivative-free optimization algorithms by quadratic
approximation to compute the maximum likelihood estimates
(<a href="http://CRAN.R-project.org/package=minqa">Bates et al. 2012</a>). Denote \(J(\mathbf \xi) = - \log L = - \log \sum_{i =
  1}^n L_i\).  Then maximizing the likelihood is equivalent to minimizing
the target function \(J(\mathbf \xi)\). Under an MAR assumption, we fix
\(\mathbf \xi_s = \mathbf 0\), while under MNAR assumption, $\mathbf \xi_s
$ can be chosen as desired.</p>

<p>During each step of the algorithm, \(\Delta_{ij}\) has to be calculated
for each subject and at each time, as well as partial derivatives for
each parameter.</p>

<p>As an example of the speed of the algorithm, for 100 bivariate
outcomes and 5 covariates, it takes about 1.9 seconds to get
convergence using R version 2.15.3 (2013-03-01) (<a href="http://www.R-project.org/"></a>) and
platform: x86_64-apple-darwin9.8.0/x86_64 (64-bit). Main parts of
the algorithm are coded in Fortran such as calculation of numerical
derivatives and log-likelihood to quicken computations. We have
incorporated those functions implementing the algorithm into the new R
(<a href="http://www.R-project.org/"></a>) package ``qrmissing&#39;&#39;.</p>

<p>We use the bootstrap \citep{efron1979,efron1993,divison1997} to
construct confidence interval and make inferences.  We resample
subjects and use bootstrap percentile intervals to form confidence
intervals.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-74" style="background:;">
  <hgroup>
    <h2>Goodness of Fit Check</h2>
  </hgroup>
  <article>
    <p>A simple goodness-of-fit check can be done by examining normal QQ
plots of the fitted residuals from the model. The visual test can help
to diagnose if the parametric assumptions are suitable for model.</p>

<p>After obtaining the MLE, we use the approach described in section
\ref{sec:deltacal} to get the fitted \(\Delta_{ij}\) for each
subject. Then the fitted residuals can be obtained by plugging in the
fitted estimates and \(\hat{\Delta}_{ij}\) to obtain,
\[
  \hat{\epsilon}_{ij} =
  \begin{cases}
    (y_{ij} - \hat{\Delta}_{ij} - \mathbf{x_{ij}^T
    \hat{\beta}_1^{(k)}})/\hat{\sigma}_1^{(k)},& j = 1 \\
    (y_{ij} - \hat{\Delta}_{ij} - \mathbf{y_{ij^{-}}^T
    \hat{\beta}_{y,j-1}^{(\geq j)}})/\hat{\sigma}_j^{(\geq j)},& j >
    1
  \end{cases}.
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-75" style="background:;">
  <hgroup>
    <h2>Bayesian MCMC</h2>
  </hgroup>
  <article>
    <p>For Bayesian inference, we specify priors on the parameters \(\mathbf
\xi\) and use a block Gibbs sampling method to draw samples from the
posterior distribution. Denote all the parameters to sample as :
\[
\begin{align*}
  \mathbf \xi_m &= \left\{ \mathbf \gamma_j,
    \mathbf \beta_{y,j-1}^{(\geq j)}, \mathbf \alpha_j^{(\geq j)} \right\}
  \mbox{ for } j = 1, \ldots, J ,\\
  \mathbf \xi_s &= \left\{ \mathbf h_j^{(k)}, \mathbf \eta_{j-1}^{(k)},  \delta_j^{(k)}
  \right\}
  \mbox{ for } k = 1, \ldots, j-1; 2 \leq j \leq J.
\end{align*}
\]
Comma separated parameters are marked to sample as a block.  Updates
of \(\mathbf \xi_m\) require a Metropolis-Hasting algorithm, while
\(\mathbf \xi_s\) samples are drawn directly from priors as desired for
missingness mechanism assumptions.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-76" style="background:;">
  <hgroup>
    <h2>Updating</h2>
  </hgroup>
  <article>
    <p>As mentioned in section \ref{sec:sa}, MAR or MNAR assumptions are
implemented via specific priors. For example, if MAR is assumed with
no uncertainty, then $ \mathbf \xi _s= \mathbf 0$ with probability 1.</p>

<p>Details for updating parameters are:</p>

<ul>
<li><p>$\mathbf \gamma_{1} $: Use Metropolis-Hasting algorithm.
\begin{enumerate}</p>

<ul>
<li>Draw (\(\mathbf \gamma_{1}^c\)) candidates from candidate
distribution;</li>
<li>Based on the new candidate parameter \(\mathbf \xi^c\), calculate
candidate \(\Delta_{ij}^c\) for each subject \(i\) as we described in
section \ref{sec:deltacal}.</li>
<li>Plug in \(\Delta_{ij}^c\) in likelihood (\ref{eq:ll}) to get
candidate likelihood;</li>
<li>Compute Metropolis-Hasting ratio, and accept the candidate
value or keep the previous value.</li>
</ul></li>
<li><p>For the rest of the identifiable parameters, algorithms for
updating the samples are all similar to \(\mathbf \gamma_j\).</p></li>
<li><p>For sensitivity parameters, because we do not get any
information from the data, we sample them from priors, which are
specified based on assumptions about the missingness.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-77" style="background:;">
  <hgroup>
    <h2>Simulation Study</h2>
  </hgroup>
  <article>
    <h3>Design , Introduce candidates</h3>

<p>In this section, we compare the performance of our proposed model in
section \ref{sec:settings} with the <em>rq</em> function (noted as RQ) in
<em>quantreg</em> R package (<a href="http://CRAN.R-project.org/package=quantreg">Koenker, 2012</a>) and Bottai&#39;s algorithm
(<a href="http://dx.doi.org/10.2427/8758">Bottai &amp; Zhen, 2013</a>) (noted as BZ). The <em>rq</em> function
minimizes the loss (check) function \(\sum_{i=1}^n \rho_{\tau} (y_i -
\mathbf x_i^T \mathbf \beta)\) in terms of \(\mathbf \beta\), where the loss
function \(\rho_{\tau} (u) = u(\tau - I(u < 0))\) and does not make any
distributional assumptions. <a href="http://dx.doi.org/10.2427/8758">Bottai &amp; Zhen (2013)</a> impute missing
outcomes using the estimated conditional quantiles of missing outcomes
given observed data. Their approach does not make distributional
assumptions similar to <em>rq</em> and assumes ignorable missing data.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-78" style="background:;">
  <hgroup>
    <h2>Design</h2>
  </hgroup>
  <article>
    <p>We considered three scenarios corresponding to both MAR and MNAR
assumptions for a bivariate response.  In the first scenario, \(Y_2\)
were missing at random and we used the MAR assumption in our
algorithm. In the next two scenarios, \(Y_2\) were missing not at
random. However, in the second scenario, we misspecified the MDM for
our algorithm and still assumed MAR, while in the third scenario, we
used the correct MNAR MDM. For each scenario, we considered three
error distributions: normal, student t distribution with 3 degrees of
freedom and Laplace distribution. For each error model, we simulated
100 data sets. For each set there are 200 bivariate observations
\(\mathbf Y_i = (Y_{i1}, Y_{i2})\) for \(i = 1, \ldots, 200\). \(Y_{i1}\)
were always observed, while some of \(Y_{i2}\) were missing. A single
covariate \(x\) was sampled from Uniform(0,2). The three models for the
full data response \(\mathbf Y_i\) were:
\begin{align<em>}
  Y<em>{i1} | R = 1 &amp; \sim 2 + x_i +  \epsilon</em>{i1} , \
  Y<em>{i1}| R = 0 &amp; \sim  -2 - x_i +  \epsilon</em>{i1} , \
  Y<em>{i2}| R = 1, Y</em>{i1}&amp;\sim 1 - x<em>i - 1/2Y</em>{i1} + \epsilon_{i2},
\end{align</em>}
where \(\epsilon_{i1}, \epsilon_{i2} \sim \textrm{N}(0, 1)\), \(t_3\) or
\(\mbox{LP}(\mbox{rate} = 1)\) distribution within each scenario.</p>

<p>For all cases, \(Pr (R = 1) = 0.5\).  When \(R = 0\), \(Y_{i2}\) is not
observed, so \(p(Y_{i2}| R = 0, Y_{i1})\) is not identifiable from
observed data.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-79" style="background:;">
  <hgroup>
    <h2>Explain</h2>
  </hgroup>
  <article>
    <p>In the first scenario, \(Y_2\) is missing at random, thus $p(Y<em>{i2} |
R = 0, Y</em>{i1}) = p(Y<em>{i2}|R = 1, Y</em>{i1}) $. In the last two
scenarios, \(Y_2\) are missing not at random. We assume \(Y_{i2}| R = 0,
Y_{i1} \sim 3 - x_i - 1/2Y_{i1} + \epsilon_{i2}\). Therefore, there is
a shift of 2 in the intercept between \(p(Y_2|R = 1, Y_1)\) and
\(p(Y_2|R = 0, Y_1)\).</p>

<p>Under an MAR assumption, the sensitivity parameter \(\mathbf \xi_s\) is
fixed at \(\mathbf 0\) as discussed in section \ref{sec:sa}. For
<em>rq</em> function from <em>quantreg</em> R package, because only
\(Y_{i2}|R = 1\) is observed, the quantile regression for \(Y_{i2}\) can
only be fit from the information of \(Y_{i2}|R = 1\) vs \(x\).</p>

<p>In scenario 2 under MNAR, we mis-specified the MDM using the wrong
sensitivity parameter \(\mathbf \xi_s\) at \(\mathbf 0\). In scenario 3, we
assumed there was an intercept shift between distribution of
\(Y_{i2}|Y_{i1}, R = 1\) and \(Y_{i2}|Y_{i1}\), \(R = 0\), thus fixed
\(\mathbf \xi_s\) at its true value.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-80" style="background:;">
  <hgroup>
    <h2>Evaluation</h2>
  </hgroup>
  <article>
    <p>For each dataset, we fit quantile regression for quantiles \(\tau =\)
0.1, 0.3, 0.5, 0.7, 0.9.  Parameter estimates were evaluated by mean
squared error (MSE),
\[
  \mbox{MSE} (\gamma_{ij}) = \frac{1}{100} \sum_{k = 1}^{100}
  \left( \hat{\gamma}_{ij}^{(k)}  - \gamma_{ij}\right)^2, i = 0, 1
\]
where \(\gamma_{j}\) is the true value for quantile regression
coefficient, \(\hat{\gamma}_{j}^{(k)}\) is the maximum likelihood
estimates in \(k\)-th simulated dataset (\((\gamma_{01}, \gamma_{11})\)
for \(Y_{i1}\), \((\gamma_{02}, \gamma_{12})\) for \(Y_{i2}\)).</p>

<p>Monte Carlo standard error (MCSE) is used to evaluate the significance
of difference between methods. It is calculated by
\[
  \mbox{MCSE} = \hat{\mbox{sd}}(\mbox{Bias}^2)/\sqrt{N},
\]
where \(\hat{\mbox{sd}}\) is the sample standard deviation and
\(\mbox{Bias} = \hat{\gamma}_{ij} - \gamma_{ij}\) and \(N\) is the number
of simulations.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-81" style="background:;">
  <hgroup>
    <h2>Results</h2>
  </hgroup>
  <article>
    <ul>
<li>[ ] may convert the result table into graph</li>
<li>[ ] comments</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-82" style="background:;">
  <hgroup>
    <h2>Goodness of Fit Check</h2>
  </hgroup>
  <article>
    <p>To assess the goodness of fit, we examined the QQ plot of fitted
residuals in model (\ref{eq:model}) to check the normality assumption
on the error term for a random sample of the simulated datasets
(Appendix \ref{sec:gofsim}).  When our error assumption is correct
(normal), the QQ plot reflects the fitted residuals follow exact a
normal distribution. However, when we misspecified the error
distribution, the proposed diagnostic method did clearly suggest
heavier tail error than normal, and this also demonstrates why our
approach has some disadvantages for regression on extreme quantiles
when errors are not normal.</p>

<ul>
<li>[ ] insert sample QQ plots here</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-83" style="background:;">
  <hgroup>
    <h2>Real Data Analysis: Tours</h2>
  </hgroup>
  <article>
    <p>We apply our quantile regression approach to data from TOURS, a weight
management clinical trial (<a href="">Perri et al. 2008</a>).  This trial was
designed to test whether a lifestyle modification program could
effectively help people to manage their weights in the long
term. After finishing the six-month weight loss program, participants
were randomly assigned to three treatments groups: face-to-face
counseling, telephone counseling and control group. Their weights were
recorded at baseline (\(Y_0\)), 6 months (\(Y_1\)) and 18 months
(\(Y_2\)). Here, we are interested in how the distribution of weights at
six months and eighteen months change with covariates. The
regressors of interest include AGE, RACE (black and white) and weight
at baseline (\(Y_0\)). Weights at the six months (\(Y_1\)) were always
observed and 13 out of 224 observations (6\%) were missing at 18
months (\(Y_2\)). The ``Age&#39;&#39; covariate was scaled to 0 to 5 with every
increment representing 5 years.</p>

<p>We fitted regression models for bivariate responses \(\mathbf Y_i =
(Y_{i1}, Y_{i2})\) for quantiles (10\%, 30\%, 50\%, 70\%, 90\%).  We
ran 1000 bootstrap samples to obtain 95\% confidence intervals.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-84" style="background:;">
  <hgroup>
    <h2>Results (Table is too large)</h2>
  </hgroup>
  <article>
    <table><thead>
<tr>
<th>Term</th>
<th>Intercept</th>
<th>Age</th>
<th>White</th>
<th>BaseWeight</th>
</tr>
</thead><tbody>
<tr>
<td>6 months</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>10\%</td>
<td>-6.05(-10.88,2.67)</td>
<td>0.34(-0.25,0.85)</td>
<td>-3.86(-5.75,-2.43)</td>
<td>0.92(0.85,0.97)</td>
</tr>
<tr>
<td>30\%</td>
<td>-2.56(-7.67,3.66)</td>
<td>0.33(-0.25,0.84)</td>
<td>-3.90(-5.43,-2.54)</td>
<td>0.92(0.86,0.97)</td>
</tr>
<tr>
<td>50\%</td>
<td>-0.25(-5.29,5.60)</td>
<td>0.31(-0.25,0.85)</td>
<td>-4.04(-5.57,-2.55)</td>
<td>0.93(0.87,0.98)</td>
</tr>
<tr>
<td>70\%</td>
<td>1.79(-3.27,7.81)</td>
<td>0.35(-0.22,0.86)</td>
<td>-4.11(-5.67,-2.68)</td>
<td>0.93(0.87,0.98)</td>
</tr>
<tr>
<td>90\%</td>
<td>4.81(-0.05,11.32)</td>
<td>0.40(-0.20,0.94)</td>
<td>-4.07(-5.68,-2.68)</td>
<td>0.94(0.88,0.99)</td>
</tr>
<tr>
<td>18 months(MAR)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>10\%</td>
<td>-17.65(-31.75,21.41)</td>
<td>-0.73(-1.99,0.39)</td>
<td>-0.12(-10.60,2.96)</td>
<td>1.01(0.63,1.14)</td>
</tr>
<tr>
<td>30\%</td>
<td>-18.26(-28.27,9.88)</td>
<td>-0.74(-2.01,0.32)</td>
<td>1.07(-8.93,3.67)</td>
<td>1.07(0.79,1.17)</td>
</tr>
<tr>
<td>50\%</td>
<td>-12.72(-24.20,10.45)</td>
<td>-0.73(-2.01,0.30)</td>
<td>1.04(-6.14,3.94)</td>
<td>1.06(0.83,1.17)</td>
</tr>
<tr>
<td>70\%</td>
<td>-9.12(-19.69,14.38)</td>
<td>-0.73(-2.00,0.31)</td>
<td>1.18(-5.18,3.92)</td>
<td>1.06(0.84,1.16)</td>
</tr>
<tr>
<td>90\%</td>
<td>-3.90(-12.65,19.61)</td>
<td>-0.75(-1.98,0.36)</td>
<td>1.24(-4.19,3.76)</td>
<td>1.08(0.85,1.16)</td>
</tr>
</tbody></table>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-85" style="background:;">
  <hgroup>
    <h2>Comments</h2>
  </hgroup>
  <article>
    <p>Estimates under MAR and MNAR are presented in Table
\ref{tab:tours}. For weights of participants at six months, weights of
whites are generally 4kg lower than those of blacks for all quantiles,
and the coefficients of race are negative and significant. Meanwhile,
weights of participants are not affected by age since the coefficients
are not significant. Difference in quantiles are reflected by the
intercept.  Coefficients of baseline weight show a strong relationship
with weights after 6 months.</p>

<p>For weights at 18 months after baseline, we have similar results.
Weights after 18 months still have a strong relationship with baseline
weights. However, whites do not weigh significantly less than blacks
at 18 months unlike at 6 months.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-86" style="background:;">
  <hgroup>
    <h2>Sensitivity Analysis</h2>
  </hgroup>
  <article>
    <p>We also did a sensitivity analysis based on an assumption of missing
not at random.  Based on previous studies of pattern of weight regain
after lifestyle treatment \citep{wadden2001, perri2008extended}, we
assume that
\[
  E(Y_2 - Y_1| R=0) = 3.6 \mbox{kg},
\]
which corresponds to 0.3kg regain per month after finishing the
initial 6-month program. We incorporate the sensitivity parameters in
the distribution of \(Y_2|Y_1, R=0\) via the following restriction:
\[
  \Delta_{i2} + \mathbf x_{i2}^T \mathbf h_2^{(1)} + E(y_{i1}|R=0)(\beta_{y,1}^{(1)} +\eta_1^{(1)} - 1) = 3.6 \mbox{kg}.
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-87" style="background:;">
  <hgroup>
    <h2>Results</h2>
  </hgroup>
  <article>
    <table><thead>
<tr>
<th>Term</th>
<th>Intercept</th>
<th>Age</th>
<th>White</th>
<th>BaseWeight</th>
</tr>
</thead><tbody>
<tr>
<td>18 months(MNAR)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>10\%</td>
<td>-20.51(-30.97,25.00)</td>
<td>-0.69(-2.23,0.47)</td>
<td>0.24(-10.19,3.04)</td>
<td>1.04(0.62,1.14)</td>
</tr>
<tr>
<td>30\%</td>
<td>-18.04(-27.14,8.73)</td>
<td>-0.74(-2.04,0.49)</td>
<td>1.08(-9.22,3.94)</td>
<td>1.07(0.83,1.16)</td>
</tr>
<tr>
<td>50\%</td>
<td>-12.19(-22.52,8.79)</td>
<td>-0.73(-2.06,0.38)</td>
<td>1.05(-6.36,4.17)</td>
<td>1.06(0.86,1.16)</td>
</tr>
<tr>
<td>70\%</td>
<td>-7.89(-17.63,12.26)</td>
<td>-0.73(-1.95,0.32)</td>
<td>1.17(-4.43,4.20)</td>
<td>1.06(0.87,1.16)</td>
</tr>
<tr>
<td>90\%</td>
<td>-3.11(-8.60,21.70)</td>
<td>-0.73(-2.02,0.38)</td>
<td>1.68(-3.90,4.05)</td>
<td>1.10(0.87,1.15)</td>
</tr>
</tbody></table>

<p>Table \ref{tab:tours} presents the estimates and bootstrap percentile
confidence intervals under the above MNAR mechanism. There are not
large differences for estimates for \(Y_2\) under MNAR vs MAR. This is
partly due to the low proportion of missing data in this study.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-88" style="background:;">
  <hgroup>
    <h2>Goodness of Fit Check</h2>
  </hgroup>
  <article>
    <p>We also checked the goodness of fit via QQ plots on the fitted
residuals as described in section \ref{sec:goodness} for each quantile
regression fit.  Plots are presented in Appendix
\ref{sec:goftours}. The QQ plots showed minimal evidence against the
assumption that the residuals were normally distributed; thus we were
confident with the conclusion of our quantile regression models.</p>

<ul>
<li>[ ] insert sample GoF graphs here</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-89" style="background:;">
  <hgroup>
    <h2>Discussion</h2>
  </hgroup>
  <article>
    <p>In this paper, we have developed a marginal quantile regression model
for data with monotone missingness. We use a pattern mixture model to
jointly model the full data response and missingness. Here we estimate
marginal quantile regression coefficients instead of coefficients
conditional on random effects as in <a href="http://dx.doi.org/10.1111/j.1541-0420.2009.01269.x">Yuan &amp; Yin (2010)</a>. In addition, our
approach allows non-parallel quantile lines over different quantiles
via the mixture distribution and allows for sensitivity analysis which
is essential for the analysis of missing data (NAS 2010).</p>

<p>Our method allows the missingness to be non-ignorable.  We illustrated
how to put informative priors for Bayesian inference and how to find
sensitivity parameters to allow different missing data mechanisms in
general. The recursive integration algorithm simplifies computation
and can be easily implemented even in high dimensions. Simulation
studies demonstrates that our approach has smaller MSE than the
traditional frequentist method <em>rq</em> function for most cases,
especially for inferences of partial missing responses. And it has
advantages over Bottai&#39;s appraoch for middle quantiles regression
inference even when the distribution is mis-specified.  However, our
approach also shows little bias for extreme quantiles comparing to
<a href="http://dx.doi.org/10.2427/8758">Bottai &amp; Zhen (2013)</a> if error is mis-specified. We also illustrated
sensitivity analysis and how to allow non-ignorable missingness
assumptions.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-90" style="background:;">
  <hgroup>
    <h2>Future Work</h2>
  </hgroup>
  <article>
    <p>Our model assumes a multivariate normal distribution for each
component in the pattern mixture model, which might be too restrictive
in some settings. Simulation results showed that the mis-specification
of the error term did have a small impact on the extreme quantile
regression inferences.  It is possible to replace that with a
semi-parametric model, for example, a Dirichlet process mixture or
Polya trees. However, computational algorithms would need to be
developed.  Meanwhile, even though we use a multivariate normal
distributions within patterns, which can easily departures from MAR
via differences in means and (co)-variances, we still need strong
assumptions for sequential multivariate normal distribution within
each pattern; otherwise MAR constraints do not exist (<a href="http://dx.doi.org/10.1111/j.1541-0420.2011.01565.x">Wang &amp; Daniels, 2011</a>).</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-91" style="background:;">
  <hgroup>
    <h2>References</h2>
  </hgroup>
  <article>
    <ul>
<li>Matteo Bottai, Huiling Zhen,   (2013) Multiple Imputation Based on Conditional Quantile Estimation.  <em>Epidemiology, Biostatistics and Public Health</em></li>
<li>Michael Daniels, Joseph Hogan,   (2008) Missing data in longitudinal studies.</li>
<li>Edward George, Robert McCulloch,   (1993) Variable Selection Via Gibbs Sampling.  <em>Journal of the American Statistical Association</em></li>
<li>T. Hanson, W.O. Johnson,   (2002) Modeling regression error with a mixture of Polya trees.  <em>Journal of the American Statistical Association</em></li>
<li>Patrick Heagerty,   (1999) Marginally Specified Logistic-Normal Models for Longitudinal Binary Data.  <em>Biometrics</em></li>
<li>A. Jara, T.E. Hanson, E. Lesaffre,   (2009) Robustifying generalized linear mixed models using a new class of mixtures of multivariate Polya trees.  <em>Journal of Computational and Graphical Statistics</em></li>
<li>Roger Koenker,  Gilbert, Jr. Bassett,   (1978) Regression Quantiles.  <em>Econometrica</em></li>
<li>A. Kottas, M. Krnjajic,   (2009) Bayesian semiparametric modelling in quantile regression.  <em>Scandinavian Journal of Statistics</em></li>
<li>Hideo Kozumi, Genya Kobayashi,   (2011) Gibbs sampling methods for {B}ayesian quantile regression.  <em>J. Stat. Comput. Simul.</em></li>
<li>Roderick Little,   (1993) Pattern-Mixture Models for Multivariate Incomplete Data.  <em>Journal of the American Statistical Association</em></li>
<li>Roderick Little,   (1994) A class of pattern-mixture models for normal incomplete data.  <em>Biometrika</em></li>
<li>Douglas Bates, Katharine Mullen, John Nash, Ravi Varadhan,   (2012) minqa: Derivative-free optimization algorithms by quadratic
approximation.</li>
<li>G. Molenberghs, B. Michiels, M. Kenward, P. Diggle,   (1998) Monotone missing data and pattern-mixture models.  <em>Statist. Neerlandica</em></li>
<li>Michael Perri, Marian Limacher, Patricia Durning, David Janicke, Lesley Lutes, Linda Bobroff, Martha Dale, Michael Daniels, Tiffany Radcliff, A Martin,   (2008) Extended-care programs for weight management in rural communities: the treatment of obesity in underserved rural settings (TOURS) randomized trial.  <em>Archives of internal medicine</em></li>
<li>Roger Koenker,   (2012) quantreg: Quantile Regression.</li>
<li>R Core Team ,   (2013) R: A Language and Environment for Statistical Computing.</li>
<li>B.J. Reich, H.D. Bondell, H.J. Wang,   (2010) Flexible Bayesian quantile regression for independent and clustered data.  <em>Biostatistics</em></li>
<li>Jason Roy,   (2003) Modeling longitudinal data with nonignorable dropouts using a
          latent dropout class model.  <em>Biometrics</em></li>
<li>Jason Roy, Michael Daniels,   (2008) A general class of pattern mixture models for nonignorable
          dropout with many possible dropout times.  <em>Biometrics</em></li>
<li>Donald Rubin,   (1977) Formalizing subjective notions about the effect of
          nonrespondents in sample surveys.  <em>J. Amer. Statist. Assoc.</em></li>
<li>S. Tokdar, J.B. Kadane,   (2011) Simultaneous linear quantile regression: A semiparametric bayesian approach.  <em>Bayesian Analysis</em></li>
<li>Stephen Walker, Bani Mallick,   (1999) A Bayesian Semiparametric Accelerated Failure Time Model.  <em>Biometrics</em></li>
<li>Chenguang Wang, Michael Daniels,   (2011) A note on {MAR}, identifying restrictions, model comparison,
          and sensitivity analysis in pattern mixture models with and
          without covariates for incomplete data.  <em>Biometrics</em></li>
<li>Ying Wei, Yanyuan Ma, Raymond Carroll,   (2012) Multiple imputation in quantile regression.  <em>Biometrika</em></li>
<li>Keming Yu, Rana Moyeed,   (2001) Bayesian quantile regression.  <em>Statistics &amp; Probability Letters</em></li>
<li>Ying Yuan, Guosheng Yin,   (2010) Bayesian quantile regression for longitudinal studies with
          nonignorable missing data.  <em>Biometrics</em> NA NA NA NA</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>

  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
<!-- Grab CDN jQuery, fall back to local if offline -->
<script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
<script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery-1.7.min.js"><\/script>')</script>
<!-- Load Javascripts for Widgets -->
<!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->
<script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<script src='libraries/widgets/bootstrap/js/bootstrap.min.js'></script>
<script>  
$(function (){ 
  $("#example").popover(); 
  $("[rel='tooltip']").tooltip(); 
});  
</script>  
<!-- LOAD HIGHLIGHTER JS FILES -->
<script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!-- DONE LOADING HIGHLIGHTER JS FILES -->
</html>